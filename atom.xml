<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小箱子</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.smartzhi.site/"/>
  <updated>2020-04-26T16:58:32.273Z</updated>
  <id>https://www.smartzhi.site/</id>
  
  <author>
    <name>Zhi</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Java8 HashMap源码阅读整理</title>
    <link href="https://www.smartzhi.site/2020/HashMap/"/>
    <id>https://www.smartzhi.site/2020/HashMap/</id>
    <published>2020-04-26T08:25:49.000Z</published>
    <updated>2020-04-26T16:58:32.273Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p class="description"></p><a id="more"></a><p><img src="https://gitee.com/zxz007/blogimg/raw/master/img/HashMap.svg" alt="HashMap" title="HashMap"></p><h2 id="Java-8-HashMap-特点"><a href="#Java-8-HashMap-特点" class="headerlink" title="Java 8 HashMap 特点"></a>Java 8 HashMap 特点</h2><ul><li>HashMap是数组+链表+红黑树</li><li>Key 可以为<code>null</code></li><li>非线程安全</li><li><code>fail-fast</code> ：进行遍历时如果执行HashMap的<code>remove(Object key)</code>或者<code>put(Object value)</code>方法时会快速失败，抛出异常<code>ConcurrentModificationException</code></li></ul><h2 id="HashMap操作"><a href="#HashMap操作" class="headerlink" title="HashMap操作"></a>HashMap操作</h2><h3 id="什么时候扩容"><a href="#什么时候扩容" class="headerlink" title="什么时候扩容"></a>什么时候扩容</h3><ul><li><code>size &gt; threshold</code></li><li><code>table == null || table.length == 0</code></li><li><code>链表要树化 &amp;&amp; table.length &lt; 64</code>  —-&gt; 取消树化</li></ul><h3 id="怎么扩容"><a href="#怎么扩容" class="headerlink" title="怎么扩容"></a>怎么扩容</h3><p>1 <code>newCap = (oldCap &lt; 1)</code><br>2<br>$newIndex =<br>\begin{cases}<br>oldIndex &amp; ,(e.hash \ \&amp; \ oldCap == 0)\<br>oldIndex + oldCap &amp; ,(e.hash \  \&amp; \ oldCap == 1) \<br>\end{cases}<br>$</p><h3 id="什么时候链表树化"><a href="#什么时候链表树化" class="headerlink" title="什么时候链表树化"></a>什么时候链表树化</h3><ul><li><code>链表长度 &gt; 8 &amp;&amp; table.length &gt;= 64</code>  &lt;—- put()</li><li><code>子树大小 &gt; 6</code>   &lt;—- 扩容</li></ul><h3 id="什么时候树链表化"><a href="#什么时候树链表化" class="headerlink" title="什么时候树链表化"></a>什么时候树链表化</h3><ul><li><code>root == null  || root.right == null || root.left == null || root.left.left ==null</code>  &lt;—- remove()</li><li><code>子树大小 &lt;= 6</code>   &lt;—- 扩容</li></ul><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><h3 id="HashMap-的-put-get-resize-remove-方法"><a href="#HashMap-的-put-get-resize-remove-方法" class="headerlink" title="HashMap 的 put | get | resize | remove 方法"></a>HashMap 的 put | get | resize | remove 方法</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 伪代码 </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">AbstractMap</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Map</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt;,... </span>&#123;</span><br><span class="line">    <span class="comment">// 常量</span></span><br><span class="line">    默认负载因子：<span class="number">0.75</span>(<span class="keyword">float</span>)</span><br><span class="line">    默认初始容量：<span class="number">1</span> &lt;&lt; <span class="number">4</span>  (= <span class="number">16</span>)</span><br><span class="line">    最大容量：<span class="number">1</span> &lt;&lt; <span class="number">30</span></span><br><span class="line">    最小树化容量：<span class="number">64</span></span><br><span class="line">    树化阈值：<span class="number">8</span></span><br><span class="line">    链表化阈值：<span class="number">6</span></span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------------</span><br><span class="line">    Node&lt;K,V&gt;[] table</span><br><span class="line">    Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet</span><br><span class="line">    <span class="keyword">int</span> size</span><br><span class="line">    <span class="keyword">int</span> modCount</span><br><span class="line">    <span class="keyword">float</span> loadFactor</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">// 构造方法只初始化了 capacity 和 loadFactor</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HashMap</span><span class="params">()</span> </span>&#123;...&#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">// hash</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hash</span><span class="params">(Object key)</span> </span>&#123;...&#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">// get 方法 伪代码</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> V <span class="title">get</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> table != <span class="keyword">null</span> :</span><br><span class="line">            <span class="keyword">if</span> 第一个节点first != <span class="keyword">null</span>  &amp;&amp; first.key == key :</span><br><span class="line">                <span class="keyword">return</span> 第一个节点.value</span><br><span class="line">            <span class="keyword">if</span> 下一个节点e != <span class="keyword">null</span> :</span><br><span class="line">                <span class="comment">//红黑树</span></span><br><span class="line">                <span class="keyword">if</span> first is 树节点:</span><br><span class="line">                    <span class="comment">// 比较根节点和key, 递归</span></span><br><span class="line">                    <span class="keyword">return</span> 红黑树中的节点.value </span><br><span class="line">                </span><br><span class="line">                <span class="comment">// 链表</span></span><br><span class="line">                <span class="keyword">for</span> e in 链表:</span><br><span class="line">                    <span class="keyword">if</span> e.key == key:</span><br><span class="line">                        <span class="keyword">return</span> e.value</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">// put 方法 伪代码</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> V <span class="title">put</span><span class="params">(K key, V value)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> 第一次put :</span><br><span class="line">            初始化table -- &gt; resize()</span><br><span class="line">        index = (table.length - <span class="number">1</span>) &amp; hash(key) <span class="comment">// 要存的位置</span></span><br><span class="line">        p: index上第一个节点 </span><br><span class="line">        <span class="keyword">if</span> 如果没有冲突 :</span><br><span class="line">            直接存</span><br><span class="line">        <span class="keyword">else</span> : <span class="comment">// 发生冲突</span></span><br><span class="line">            <span class="keyword">if</span> 第一个节点key相同 :</span><br><span class="line">                替换第一个节点value，返回oldvalue;</span><br><span class="line">            <span class="comment">// 红黑树</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> 第一个节点 是树节点</span><br><span class="line">                红黑树put(k,v);</span><br><span class="line">            <span class="comment">// 链表</span></span><br><span class="line">            <span class="keyword">else</span> :</span><br><span class="line">                遍历链表:</span><br><span class="line">                    <span class="keyword">if</span> 链表上e 节点key相同 :</span><br><span class="line">                        替换e.value，返回oldvalue</span><br><span class="line">                    <span class="keyword">if</span> 链表最后节点</span><br><span class="line">                        尾插法</span><br><span class="line">                        <span class="keyword">if</span> 此时链表长度 &gt; <span class="number">8</span> :</span><br><span class="line">                            链表变红黑树()</span><br><span class="line">                        <span class="keyword">break</span>       </span><br><span class="line">        &#125;</span><br><span class="line">        修改次数++</span><br><span class="line">        要扩容 -- &gt; resize()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">// 整形 resize() 初始化 or 扩容</span></span><br><span class="line">    <span class="keyword">final</span> Node&lt;K,V&gt;[] resize() &#123;</span><br><span class="line">        <span class="comment">// 1. 计算newCap 和 newThr</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="comment">// 2. 初始化table</span></span><br><span class="line">        Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])<span class="keyword">new</span> Node[newCap]</span><br><span class="line">        table = newTab</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 3. 扩容</span></span><br><span class="line">        <span class="keyword">if</span> oldtable 有节点 :</span><br><span class="line">            遍历oldtable, 第j个为 Node e : </span><br><span class="line">                <span class="keyword">if</span> e != <span class="keyword">null</span> :</span><br><span class="line">                    <span class="keyword">if</span> 只有一个节点 :</span><br><span class="line">                        直接进 table</span><br><span class="line">                    <span class="comment">// 红黑树</span></span><br><span class="line">                    <span class="keyword">else</span> <span class="keyword">if</span> e是树节点 :</span><br><span class="line">                        e.split(<span class="keyword">this</span>, newTab, j, oldCap); <span class="comment">//分树</span></span><br><span class="line">                    <span class="comment">// 链表</span></span><br><span class="line">                    <span class="keyword">else</span> :</span><br><span class="line">                        遍历链表 : <span class="comment">// 分链表--&gt; 高低链表*</span></span><br><span class="line">                            <span class="keyword">if</span> e.hash &amp; oldCap == <span class="number">0</span> :</span><br><span class="line">                                low 尾插</span><br><span class="line">                            <span class="keyword">else</span> : </span><br><span class="line">                                high 尾插</span><br><span class="line">                        table[j] = low</span><br><span class="line">                        table[j + oldCap] = high</span><br><span class="line">        <span class="keyword">return</span> newTab</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">// 树化</span></span><br><span class="line">    <span class="function"><span class="keyword">final</span> <span class="keyword">void</span> <span class="title">treeifyBin</span><span class="params">(Node&lt;K,V&gt;[] tab, <span class="keyword">int</span> hash)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> table.length &lt; <span class="number">64</span>:</span><br><span class="line">            resize() <span class="comment">// 扩容</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> 此处不为空 :</span><br><span class="line">            建立双向链表(树节点)</span><br><span class="line">            <span class="keyword">if</span> 双向链表头结点 hd != <span class="keyword">null</span> :</span><br><span class="line">                hd.treeify(tab)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">// 移除</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> V <span class="title">remove</span><span class="params">(Object key)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> table != <span class="keyword">null</span> &amp;&amp; index处第一个节点 p != <span class="keyword">null</span> :</span><br><span class="line">            声明 node: 要删除的节点 </span><br><span class="line">            <span class="keyword">if</span> 第一个节点 p.key == key</span><br><span class="line">                node = p;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> 还有下一个节点 : </span><br><span class="line">                <span class="comment">// 红黑树</span></span><br><span class="line">                <span class="keyword">if</span> p是树节点 :</span><br><span class="line">                    node = 红黑树中找node</span><br><span class="line">                <span class="comment">// 链表</span></span><br><span class="line">                <span class="keyword">else</span> :</span><br><span class="line">                    node = 链表中找node</span><br><span class="line">            <span class="keyword">if</span> node 是要删除的节点 :</span><br><span class="line">                <span class="comment">// 红黑树</span></span><br><span class="line">                <span class="keyword">if</span> node 是树节点 :</span><br><span class="line">                    红黑树中删除 node</span><br><span class="line">                <span class="comment">// 链表</span></span><br><span class="line">                <span class="keyword">else</span> :</span><br><span class="line">                    链表中删除 node</span><br><span class="line">                    </span><br><span class="line">                修改次数++</span><br><span class="line">                大小--</span><br><span class="line">                返回 node.value</span><br><span class="line">        返回 <span class="keyword">null</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Node-内部类"><a href="#Node-内部类" class="headerlink" title="Node 内部类"></a>Node 内部类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">implements</span> <span class="title">Map</span>.<span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> hash;</span><br><span class="line">    K key;</span><br><span class="line">    V value;</span><br><span class="line">    Node&lt;K,V&gt; next;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">// 构造方法</span></span><br><span class="line">    Node(<span class="keyword">int</span> hash, K key, V value, Node&lt;K,V&gt; next) &#123;...&#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> K <span class="title">getKey</span><span class="params">()</span> </span>&#123;...&#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">getValue</span><span class="params">()</span>  </span>&#123;...&#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> V <span class="title">setValue</span><span class="params">(V newValue)</span> </span>&#123;...&#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">// 同时比较key 和 value</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>&#123;...&#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123; </span><br><span class="line">        <span class="keyword">return</span> key + <span class="string">"="</span> + value; </span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> Objects.hashCode(key) ^ Objects.hashCode(value);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="TreeNode-内部类"><a href="#TreeNode-内部类" class="headerlink" title="TreeNode 内部类"></a>TreeNode 内部类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">LinkedHashMap</span>.<span class="title">Entry</span>&lt;<span class="title">K</span>,<span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    TreeNode&lt;K,V&gt; parent;  <span class="comment">// red-black tree links</span></span><br><span class="line">    TreeNode&lt;K,V&gt; left;</span><br><span class="line">    TreeNode&lt;K,V&gt; right;</span><br><span class="line">    TreeNode&lt;K,V&gt; prev;    <span class="comment">// needed to unlink next upon deletion</span></span><br><span class="line">    <span class="keyword">boolean</span> red;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">// 构造方法</span></span><br><span class="line">    TreeNode(<span class="keyword">int</span> hash, K key, V val, Node&lt;K,V&gt; next) &#123;...&#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">// get()  </span></span><br><span class="line">    <span class="function">TreeNode&lt;K,V&gt; <span class="title">getTreeNode</span><span class="params">(<span class="keyword">int</span> h, Object k)</span> </span>&#123;...&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// put()</span></span><br><span class="line">    <span class="function">TreeNode&lt;K,V&gt; <span class="title">putTreeVal</span><span class="params">(HashMap&lt;K,V&gt; map, Node&lt;K,V[] tab, <span class="keyword">int</span> h, K k, V v)</span> </span>&#123;...&#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">// 树化</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">treeify</span><span class="params">(Node&lt;K,V&gt;[] tab)</span> </span>&#123;...&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 链表化</span></span><br><span class="line">    <span class="function">Node&lt;K,V&gt; <span class="title">untreeify</span><span class="params">(HashMap&lt;K,V&gt; map)</span> </span>&#123;...&#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * HashMap 扩容时，红黑树情况</span></span><br><span class="line"><span class="comment">     *   1. 和链表类似，分为low和high</span></span><br><span class="line"><span class="comment">     *   2. 如果子树的个数 &lt;= 6, 那么该子树链表化</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">split</span><span class="params">(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, <span class="keyword">int</span> index, <span class="keyword">int</span> bit)</span> </span>&#123;...&#125;</span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 红黑树移除节点时</span></span><br><span class="line"><span class="comment">     *   如果   root == null || root.right == null </span></span><br><span class="line"><span class="comment">     *       || root.left == null || root.left.left ==null</span></span><br><span class="line"><span class="comment">     *   那么 该树链表化</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">removeTreeNode</span><span class="params">(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[]tab, <span class="keyword">boolean</span> movable)</span> </span>&#123;...&#125;</span><br><span class="line">---------------------------------------------------------------------------------------------------</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      &lt;p class=&quot;description&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Java" scheme="https://www.smartzhi.site/categories/Java/"/>
    
    
      <category term="Java" scheme="https://www.smartzhi.site/tags/Java/"/>
    
      <category term="jdk8" scheme="https://www.smartzhi.site/tags/jdk8/"/>
    
      <category term="HashMap" scheme="https://www.smartzhi.site/tags/HashMap/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow笔记（第四讲）</title>
    <link href="https://www.smartzhi.site/2019/Tensorflow04/"/>
    <id>https://www.smartzhi.site/2019/Tensorflow04/</id>
    <published>2019-09-17T02:37:52.000Z</published>
    <updated>2020-04-26T17:04:11.122Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p class="description">MNIST数据集与全连接网络</p><a id="more"></a><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="MNIST-数据"><a href="#MNIST-数据" class="headerlink" title="MNIST 数据"></a>MNIST 数据</h3><p>mnist数据集:<br>提供6W张 28*28像素点的0~9手写数字图片和标签，用于训练<br>提供1W张 28*28像素点的0~9手写数字图片和标签，用于测试</p><p>每张图片有784个像素点，组成长度为784的一维数组，作为输入特征<br>图片的标签以一维数组形式给出，每个元素表示对应分类出现的概率。</p><blockquote><p>输入</p><p>$\underbrace{0.\quad 0. \quad 0.380 \quad 0.500 \quad \cdots \quad  0. \quad 0. }_{784个}$</p><p>输出</p><p><code>[0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]</code></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'./data/'</span>, one_hot = <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># mnist = input_data.read_data_sets('F://mnist//', one_hot = True)</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"train data size: "</span>,mnist.train.num_examples)</span><br><span class="line">print(<span class="string">"validation data size: "</span>,mnist.validation.num_examples)</span><br><span class="line">print(<span class="string">"test data size: "</span>,mnist.test.num_examples)</span><br><span class="line"></span><br><span class="line">mnist.train.labels[<span class="number">0</span>]</span><br><span class="line">mnist.train.images[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">200</span></span><br><span class="line">xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">print(<span class="string">"xs shape: "</span>, xs.shape)</span><br><span class="line">print(<span class="string">"ys shape: "</span>, ys.shape)</span><br></pre></td></tr></table></figure><h3 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h3><div class="table-container"><table><thead><tr><th style="text-align:left">函数</th><th style="text-align:left">功能</th></tr></thead><tbody><tr><td style="text-align:left"><code>tf.get_collection(&quot; &quot;)</code></td><td style="text-align:left">从集合中提取全部变量，生成一个列表</td></tr><tr><td style="text-align:left"><code>tf.add_n([])</code></td><td style="text-align:left">列表内元素对应相加</td></tr><tr><td style="text-align:left"><code>tf.cast(x, dtype)</code></td><td style="text-align:left">把x 转为dtype类型</td></tr><tr><td style="text-align:left"><code>tf.argmax(x, axis)</code></td><td style="text-align:left">返回最大值所在索引号</td></tr><tr><td style="text-align:left"><code>os.path.join(&quot;home&quot;, &quot;name&quot;)</code></td><td style="text-align:left">返回 “home/name”</td></tr><tr><td style="text-align:left"><code>str.split()</code></td><td style="text-align:left">按指定分隔符对字符串切片</td></tr><tr><td style="text-align:left"><code>with tf.Graph().as_default() as g:</code></td><td style="text-align:left">其内定义的节点造计算图g中</td></tr></tbody></table></div><h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">saver= tf.train.Savert()    <span class="comment">#实例化saver对象</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:  <span class="comment">#在with结构for循环中一定轮数时 保存模型到当前会话</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        <span class="keyword">if</span> i % 轮数 == <span class="number">0</span>:    <span class="comment">#拼接成 ./MODEL_SAVE_PATH/MODEL_NAME-global_step </span></span><br><span class="line">            saver.save(sess，os.path.join(MODEL_SAVE_PATH，MODEL_NAME), global_step = global_step)</span><br></pre></td></tr></table></figure><h3 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    ckpt = tf.train.get_checkpoint_state(存储路径)</span><br><span class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">        saver.restore(sess, ckpt.model_checkpoint_path)</span><br></pre></td></tr></table></figure><h3 id="实例化可还原滑动平均值的saver"><a href="#实例化可还原滑动平均值的saver" class="headerlink" title="实例化可还原滑动平均值的saver"></a>实例化可还原滑动平均值的saver</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ema = tf.train.ExponentialMovingAverage(滑动平均基数)</span><br><span class="line">ema_restore = ema.variables_to_restore()</span><br><span class="line">saver = tf.train.Saver(ema_restore)</span><br></pre></td></tr></table></figure><h3 id="准确率计算方法"><a href="#准确率计算方法" class="headerlink" title="准确率计算方法"></a>准确率计算方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)</span><br></pre></td></tr></table></figure><h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># forward.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, regularizer)</span>:</span></span><br><span class="line">    w = </span><br><span class="line">    b = </span><br><span class="line">    y = </span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># backward.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(mnist)</span>:</span></span><br><span class="line">    x = </span><br><span class="line">    y_ = </span><br><span class="line">    y = </span><br><span class="line">    global_step = </span><br><span class="line">    loss = </span><br><span class="line">    <span class="comment"># 正则化、指数衰减学习率、滑动平均</span></span><br><span class="line">    train_step = </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实例化saver</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            sess.run(train_step, feed_dict = &#123;x: ,y_: &#125;)</span><br><span class="line">            <span class="keyword">if</span> i % 轮数 ==<span class="number">0</span>:</span><br><span class="line">                print()</span><br><span class="line">                saver.save( )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则化</span></span><br><span class="line"><span class="comment"># backward.py</span></span><br><span class="line">ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels = tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">cem = tf.reduce_mean(ce)</span><br><span class="line">loss = cem + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"><span class="comment"># forward.py</span></span><br><span class="line"><span class="keyword">if</span> regularizer != <span class="literal">None</span>:</span><br><span class="line">    tf.add_to_collection(<span class="string">"losses"</span>, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率</span></span><br><span class="line"><span class="comment"># backward.py</span></span><br><span class="line">learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP, LEARNING_RATE_DECAY, staircase = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 滑动平均 ema</span></span><br><span class="line"><span class="comment"># backward.py</span></span><br><span class="line">ema = tf.train.ExponentialMovingAverage(moving_average_decay, global_step)</span><br><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">    train_op = tf.no_op(name = <span class="string">'train'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># test.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(mnist)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g；</span><br><span class="line">        <span class="comment"># 定义 x, y_, y</span></span><br><span class="line">        <span class="comment"># 实例化可还原滑动平均值的saver</span></span><br><span class="line">        <span class="comment"># 计算正确率</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            ckpt = tf.train.get_checkpoint_state(<span class="string">'路径'</span>)</span><br><span class="line">            <span class="comment"># 加载ckpt模型</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">                saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line">                <span class="comment"># 恢复会话</span></span><br><span class="line"></span><br><span class="line">                global_step = ckpt.model_checkpoint_path.split(<span class="string">"/"</span>)[<span class="number">-1</span>].split(<span class="string">"-"</span>)[<span class="number">-1</span>]</span><br><span class="line">                <span class="comment"># 恢复轮数</span></span><br><span class="line"></span><br><span class="line">                accuracy_score = sess.run(accuracy, feed_dict = &#123;x:mnist.test.images,y_:mnist.test.labels&#125;)</span><br><span class="line">                <span class="comment"># 计算准确率</span></span><br><span class="line"></span><br><span class="line">                print(<span class="string">"After %5d training step(s), test accuracy = %g"</span>%(global_step, accuracy_score))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">"Checkpoint file not found"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"./data/"</span>, one_hot = <span class="literal">True</span>)</span><br><span class="line">    test(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><div class="table-container"><table><thead><tr><th style="text-align:left">功能</th><th style="text-align:left">名称</th></tr></thead><tbody><tr><td style="text-align:left">前向传播</td><td style="text-align:left">mnist_forward.py</td></tr><tr><td style="text-align:left">反向传播</td><td style="text-align:left">mnist_backward.py</td></tr><tr><td style="text-align:left">测试输出准确率</td><td style="text-align:left">mnist_test.py</td></tr></tbody></table></div><h3 id="mnist-forward-py"><a href="#mnist-forward-py" class="headerlink" title="mnist_forward. py"></a>mnist_forward. py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line">LAYER1_NODE = <span class="number">500</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    w = tf.Variable(tf.truncated_normal(shape, stddev = <span class="number">0.1</span>))</span><br><span class="line">    <span class="keyword">if</span> regularizer != <span class="literal">None</span>:</span><br><span class="line">        tf.add_to_collection(<span class="string">'losses'</span>, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br><span class="line">    b = tf.Variable(tf.zeros(shape))</span><br><span class="line">    <span class="keyword">return</span> b</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, regularizer)</span>:</span></span><br><span class="line">    w1 = get_weight([INPUT_NODE, LAYER1_NODE], regularizer)</span><br><span class="line">    b1 = get_bias([LAYER1_NODE])</span><br><span class="line">    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line"></span><br><span class="line">    w2 = get_weight([LAYER1_NODE, OUTPUT_NODE], regularizer)</span><br><span class="line">    b2 = get_bias([OUTPUT_NODE])</span><br><span class="line">    y = tf.matmul(y1, w2) + b2</span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><h3 id="mnist-backward-py"><a href="#mnist-backward-py" class="headerlink" title="mnist_backward. py"></a>mnist_backward. py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> mnist_forward</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">200</span></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.1</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span></span><br><span class="line">REGULARIZER = <span class="number">0.0001</span></span><br><span class="line">STEPS = <span class="number">50000</span></span><br><span class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span></span><br><span class="line">MODEL_SAVE_PATH = <span class="string">'F://mnist//model'</span></span><br><span class="line">MODEL_NAME = <span class="string">'mnist_model'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(mnist)</span>:</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="literal">None</span>, mnist_forward.INPUT_NODE])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, mnist_forward.OUTPUT_NODE])</span><br><span class="line">    y = mnist_forward.forward(x, REGULARIZER)</span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable = <span class="literal">False</span>)</span><br><span class="line">    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels = tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    cem = tf.reduce_mean(ce)</span><br><span class="line">    loss = cem + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"></span><br><span class="line">    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, mnist.train.num_examples/BATCH_SIZE, LEARNING_RATE_DECAY, staircase = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)</span><br><span class="line">    </span><br><span class="line">    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">        train_op = tf.no_op(name = <span class="string">'train'</span>)</span><br><span class="line">    </span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict = &#123;x: xs, y_: ys&#125;)</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"After %5d training step(s), loss on training batch is %g"</span>%(step, loss_value))</span><br><span class="line">                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step = global_step)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"F://mnist//"</span>, one_hot = <span class="literal">True</span>)</span><br><span class="line">    backward(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h3 id="mnist-test-py"><a href="#mnist-test-py" class="headerlink" title="mnist_test. py"></a>mnist_test. py</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time </span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf </span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> mnist_forward</span><br><span class="line"><span class="keyword">import</span> mnist_backward </span><br><span class="line">TEST_INTERVAL_SECS = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(mnist)</span>:</span> </span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default() <span class="keyword">as</span> g: </span><br><span class="line">        x = tf.placeholder(tf.float32, [<span class="literal">None</span>, mnist_forward.INPUT_NODE])</span><br><span class="line">        y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, mnist_forward.OUTPUT_NODE])</span><br><span class="line">        y = mnist_forward.forward(x, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        ema = tf.train.ExponentialMovingAverage(mnist_backward.MOVING_AVERAGE_DECAY)</span><br><span class="line">        ema_restore = ema.variables_to_restore()</span><br><span class="line">        saver = tf.train.Saver(ema_restore)</span><br><span class="line"></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">                ckpt = tf.train.get_checkpoint_state(mnist_backward.MODEL_SAVE_PATH)</span><br><span class="line">                <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">                    saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line">                    global_step = ckpt.model_checkpoint_path.split(<span class="string">"/"</span>)[<span class="number">-1</span>].split(<span class="string">"-"</span>)[<span class="number">-1</span>]</span><br><span class="line">                    accuracy_score = sess.run(accuracy, feed_dict = &#123;x:mnist.test.images,y_:mnist.test.labels&#125;)</span><br><span class="line">                    print(<span class="string">"After %5d training step(s), test accuracy = %g"</span>%(global_step, accuracy_score))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    print(<span class="string">"Checkpoint file not found"</span>)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">            time.sleep(TEST_INTERVAL_SECS)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    mnist = input_data.read_data_sets(<span class="string">"F://mnist//"</span>, one_hot = <span class="literal">True</span>)</span><br><span class="line">    test(mnist)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h3 id="断点续训-mnist-backward-py"><a href="#断点续训-mnist-backward-py" class="headerlink" title="断点续训 mnist_backward. py"></a>断点续训 mnist_backward. py</h3><figure class="highlight diff"><figcaption><span>& python</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">import mnist_forward</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = 200</span><br><span class="line">LEARNING_RATE_BASE = 0.1</span><br><span class="line">LEARNING_RATE_DECAY = 0.99</span><br><span class="line">REGULARIZER = 0.0001</span><br><span class="line">STEPS = 50000</span><br><span class="line">MOVING_AVERAGE_DECAY = 0.99</span><br><span class="line">MODEL_SAVE_PATH = 'F://mnist//model'</span><br><span class="line">MODEL_NAME = 'mnist_model'</span><br><span class="line"></span><br><span class="line">def backward(mnist):</span><br><span class="line">    x = tf.placeholder(tf.float32, [None, mnist_forward.INPUT_NODE])</span><br><span class="line">    y_ = tf.placeholder(tf.float32, [None, mnist_forward.OUTPUT_NODE])</span><br><span class="line">    y = mnist_forward.forward(x, REGULARIZER)</span><br><span class="line">    global_step = tf.Variable(0, trainable = False)</span><br><span class="line">    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels = tf.argmax(y_, 1))</span><br><span class="line">    cem = tf.reduce_mean(ce)</span><br><span class="line">    loss = cem + tf.add_n(tf.get_collection('losses'))</span><br><span class="line"></span><br><span class="line">    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, mnist.train.num_examples/BATCH_SIZE, LEARNING_RATE_DECAY, staircase = True)</span><br><span class="line">    </span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)</span><br><span class="line">    </span><br><span class="line">    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line">    ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line">    with tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">        train_op = tf.no_op(name = 'train')</span><br><span class="line">    </span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">    with tf.Session() as sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line"><span class="addition">+       ckpt = tf.train.get_checkpoint_state(MODEL_SAVE_PATH)</span></span><br><span class="line"><span class="addition">+       if ckpt and ckpt.model_checkpoint_path:</span></span><br><span class="line"><span class="addition">+           saver.restore(sess, ckpt.model_checkpoint_path)</span></span><br><span class="line"></span><br><span class="line">        for i in range(STEPS):</span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            _, loss_value, step = sess.run([train_op, loss, global_step], feed_dict = &#123;x: xs, y_: ys&#125;)</span><br><span class="line">            if i % 1000 == 0:</span><br><span class="line">                print("After %5d training step(s), loss on training batch is %g"%(step, loss_value))</span><br><span class="line">                saver.save(sess, os.path.join(MODEL_SAVE_PATH, MODEL_NAME), global_step = global_step)</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    mnist = input_data.read_data_sets("F://mnist//", one_hot = True)</span><br><span class="line">    backward(mnist)</span><br><span class="line"></span><br><span class="line">if __name__=='__main__':</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      &lt;p class=&quot;description&quot;&gt;MNIST数据集与全连接网络&lt;/p&gt;
    
    </summary>
    
      <category term="Tensorflow" scheme="https://www.smartzhi.site/categories/Tensorflow/"/>
    
    
      <category term="MOOC" scheme="https://www.smartzhi.site/tags/MOOC/"/>
    
      <category term="Tensorflow" scheme="https://www.smartzhi.site/tags/Tensorflow/"/>
    
      <category term="MNIST" scheme="https://www.smartzhi.site/tags/MNIST/"/>
    
      <category term="全连接网络" scheme="https://www.smartzhi.site/tags/%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow笔记（第三讲）</title>
    <link href="https://www.smartzhi.site/2019/Tensorflow03/"/>
    <id>https://www.smartzhi.site/2019/Tensorflow03/</id>
    <published>2019-07-11T09:14:37.000Z</published>
    <updated>2020-04-26T17:07:41.497Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><div class="note info"> 神经网络优化：损失函数、学习率、滑动平均、正则化</div><a id="more"></a><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.sigmoid()</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">f(x) = \frac{1}{1+e^{-x}}</script><p><img src="https://gitee.com/zxz007/blogimg/raw/master/img/tensenflower007.jpg" alt="tensenflower007" title="tensenflower007"></p><h3 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.tanh()</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">f(x)= \frac{1-e^{-2x}}{1+e^{-2x}}</script><p><img src="https://gitee.com/zxz007/blogimg/raw/master/img/tensenflower008.jpg" alt="tensenflower008" title="tensenflower008"></p><h3 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.relu()</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{aligned}f(x) & = max(x, 0) \\     & = \begin{cases}          0, & \text{x$\leq$ 0}\\          x,& \text{x > 0}         \end{cases}\end{aligned}</script><p><img src="https://gitee.com/zxz007/blogimg/raw/master/img/tensenflower009.jpg" alt="tensenflower009" title="tensenflower009"></p><h3 id="Leaky-Relu"><a href="#Leaky-Relu" class="headerlink" title="Leaky Relu"></a>Leaky Relu</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.leaky_relu(</span><br><span class="line">    features,</span><br><span class="line">    alpha=<span class="number">0.2</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># features：一个Tensor,表示预激活值,必须是下列类型之一：float16,float32,float64,int32,int64.</span></span><br><span class="line"><span class="comment"># alpha：x &lt;0时激活函数的斜率.</span></span><br><span class="line"><span class="comment"># name：操作的名称(可选).</span></span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{aligned}f(x)  & = max(\alpha x, x) \\      & = \begin{cases}          \alpha x, & \text{x$\leq$ 0}\\          x,& \text{x > 0}         \end{cases}\end{aligned}</script><p><img src="https://gitee.com/zxz007/blogimg/raw/master/img/tensenflower010.jpg" alt="tensenflower010" title="tensenflower010"></p><h3 id="NN-复杂度"><a href="#NN-复杂度" class="headerlink" title="NN 复杂度"></a>NN 复杂度</h3><p><img src="https://gitee.com/zxz007/blogimg/raw/master/img/tensenflower011.jpg" alt="tensenflower011" title="tensenflower011"></p><p>层数 = 2<br>总参数 = (3 * 4 + 4) + (4 * 2 + 2) = 26</p><h2 id="神经网络优化"><a href="#神经网络优化" class="headerlink" title="神经网络优化"></a>神经网络优化</h2><h3 id="损失函数-loss"><a href="#损失函数-loss" class="headerlink" title="损失函数(loss)"></a>损失函数(loss)</h3><p>NN 优化目标： loss最小<br>loss: MSE、 CE、 自定义</p><h4 id="均方误差-MSE"><a href="#均方误差-MSE" class="headerlink" title="均方误差 MSE"></a>均方误差 MSE</h4><script type="math/tex; mode=display">MSE(y_-,y)=\frac{\sum_{i=1}^{n}{(y-y_-)}^2}{n}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_mse = tf.reduce_mean(tf.square(y_-y))</span><br></pre></td></tr></table></figure><blockquote><p>一个例子：</p><p>预测酸奶日销量y。x1，x2是影响日销量的因素</p><p>建模前，应预先采集的数据有：每日x1，x2和销量y_（即已知答案，最佳情况：产量 = 销量）</p><p>拟造数据集X，Y<em>，y</em> = x1+x2 </p><p>噪声，-0.05 ~ +0.05 拟合可以预测销量的函数</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">BATCH_SIZE = <span class="number">8</span></span><br><span class="line">SEED = <span class="number">23455</span></span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(<span class="number">32</span>, <span class="number">2</span>)</span><br><span class="line">Y_ = [[x1 + x2 + (rdm.rand() / <span class="number">10.0</span> - <span class="number">0.05</span>)]  <span class="keyword">for</span> (x1,  x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入、参数、输出，定义前向传播</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape = (<span class="literal">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape = (<span class="literal">None</span>, <span class="number">1</span>))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>], stddev = <span class="number">1</span>, seed = <span class="number">1</span>))</span><br><span class="line">y = tf.matmul(x, w1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数，定义后向传播</span></span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y_ - y))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss_mse)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话，训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer() </span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = <span class="number">20000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i * BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict = &#123;x: X[start: end], y_: Y_[start: end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> ==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"After %5d training step(s), w1 is: "</span>%(i))</span><br><span class="line">            print(sess.run(w1),<span class="string">"\n"</span>)</span><br><span class="line">    print(<span class="string">"Final w1 is: \n"</span>, sess.run(w1))</span><br><span class="line"></span><br><span class="line"><span class="comment">#  Final w1 is: </span></span><br><span class="line"><span class="comment">#   [[0.98019385]</span></span><br><span class="line"><span class="comment">#   [1.0159807 ]]</span></span><br></pre></td></tr></table></figure><h4 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h4><blockquote><p>如预测商品销量，预测多了，损失成本；预测少了，损失利润。</p><p>若利润$\neq$成本，则mse产生的loss无法利益最大化。</p></blockquote><script type="math/tex; mode=display">loss(y_-,y)=\sum_{n}{f(y_-,y)}</script><script type="math/tex; mode=display">\begin{aligned}f(y_-,y)  & = \begin{cases}          PROFIT \times (y_--y) , & y<y_-\\          COST \times (y-y_-) ,& y\geq y_-         \end{cases}\end{aligned}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_sum(tf.where(tf.greater(y, y_), COST * (y - y_), PROFIT * (y_ - y)))</span><br></pre></td></tr></table></figure><blockquote><p>如：预测酸奶销量，酸奶成本（COST）1元，奶利润（PROFIT）9元。</p><p>预测少了损失利润9元，大于预测多了损失成本1元。</p><p>预测多了损失大，希望生成的预测函数往多了预测。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">BATCH_SIZE = <span class="number">8</span></span><br><span class="line">SEED = <span class="number">23455</span></span><br><span class="line">COST = <span class="number">1</span></span><br><span class="line">PROFIT = <span class="number">9</span></span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(SEED)</span><br><span class="line">X = rdm.rand(<span class="number">32</span>, <span class="number">2</span>)</span><br><span class="line">Y_ = [[x1 + x2 + (rdm.rand() / <span class="number">10.0</span> - <span class="number">0.05</span>)]  <span class="keyword">for</span> (x1,  x2) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入、参数、输出，定义前向传播</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape = (<span class="literal">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape = (<span class="literal">None</span>, <span class="number">1</span>))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">1</span>], stddev = <span class="number">1</span>, seed = <span class="number">1</span>))</span><br><span class="line">y = tf.matmul(x, w1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数，定义后向传播</span></span><br><span class="line">loss = tf.reduce_sum(tf.where(tf.greater(y, y_), COST * (y - y_), PROFIT * (y_ - y)))</span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成会话，训练</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer() </span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = <span class="number">20000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i * BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict = &#123;x: X[start: end], y_: Y_[start: end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">500</span> ==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"After %5d training step(s), w1 is: "</span>%(i))</span><br><span class="line">            print(sess.run(w1),<span class="string">"\n"</span>)</span><br><span class="line">    print(<span class="string">"Final w1 is: \n"</span>, sess.run(w1))</span><br><span class="line"></span><br><span class="line"><span class="comment">#  Final w1 is: </span></span><br><span class="line"><span class="comment">#   [[1.020171 ]</span></span><br><span class="line"><span class="comment">#   [1.0425103]]</span></span><br></pre></td></tr></table></figure><h4 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h4><p>表征两个概率分布之间的距离</p><script type="math/tex; mode=display">H(y_-,y) = - \sum y_{-}\log{y}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ce = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, <span class="number">1e-12</span>, <span class="number">1.0</span>)))</span><br><span class="line"><span class="comment"># 函数将输入log 的值做了限制，</span></span><br><span class="line"><span class="comment"># 保证：y小于1e-12时，取1e-12；y大于1.0时，取1.0</span></span><br></pre></td></tr></table></figure><p>当n分类的n个输出$(y_1, y_2, \cdots ,y_n)$ 通过softmax() 函数，便满足概率分布要求：</p><script type="math/tex; mode=display">\forall x \ P(X=x) \in [0,1] \quad \wedge \quad \sum_x{P(X=x)=1}</script><script type="math/tex; mode=display">softmax(y_i) = \frac{e^{y_i}}{\sum_{j=1}^{n}{e^{y_i}}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels = tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">cem = tf.reduce_mean(ce)</span><br></pre></td></tr></table></figure><h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><script type="math/tex; mode=display">w_{n+1} = w_n - learning\_rate \nabla</script><blockquote><p>$ w_{n+1} $ : 更新后的参数</p><p>$w_n$ : 当前参数</p><p>$learning_rate$ : 每次参数更新的幅度, 学习率大会导致振荡不收敛，学习率小导致收敛速度慢</p><p>$\nabla$ : 损失函数的梯度</p><p>$\nabla = \dfrac{\partial loss}{\partial w}$</p></blockquote><h4 id="指数衰减学习率"><a href="#指数衰减学习率" class="headerlink" title="指数衰减学习率"></a>指数衰减学习率</h4><script type="math/tex; mode=display">learning\_rate = base + decay^{\frac{global\_step}{learning\_rate\_step}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(<span class="number">0</span>, trainable = <span class="literal">False</span>)</span><br><span class="line">learning_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    LEARNING_RATE_STEP,</span><br><span class="line">    LEARNING_RATE_DECAY,</span><br><span class="line">    staircase = <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># LEARNING_RATE_BASE: 学习率基数</span></span><br><span class="line"><span class="comment"># LEARNING_RATE_DECAY: 学习率衰减率 (0,1)</span></span><br><span class="line"><span class="comment"># global_step: 运行 BATCH_SIZE 次数</span></span><br><span class="line"><span class="comment"># LREANING_RATE_STEP: 更新学习率需要的轮数 = 总样本数 / BATCH_SIZE</span></span><br><span class="line"><span class="comment"># staircase —— True: global_step / learning_rate_step 取整数, 学习率离散</span></span><br><span class="line"><span class="comment">#              False: 学习率连续</span></span><br></pre></td></tr></table></figure><p>例子：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义初始学习率、学习衰减率、更新学习率所需轮数</span></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.1</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span></span><br><span class="line">LEARNING_RATE_STEP = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行BATCH_SIZE 计数器，不训练</span></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>, trainable = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义指数下降学习率</span></span><br><span class="line">learning_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE,</span><br><span class="line">    global_step,</span><br><span class="line">    LEARNING_RATE_STEP,</span><br><span class="line">    LEARNING_RATE_DECAY,</span><br><span class="line">    staircase = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义待优化参数、损失函数</span></span><br><span class="line">w = tf.Variable(tf.constant(<span class="number">5</span>, dtype = tf.float32))</span><br><span class="line">loss = tf.square(w + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义反向传播</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 会话</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">40</span>):</span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        learning_rate_val = sess.run(learning_rate)</span><br><span class="line">        global_step_val = sess.run(global_step)</span><br><span class="line">        w_val = sess.run(w)</span><br><span class="line">        loss_val = sess.run(loss)</span><br><span class="line">        print(<span class="string">"After %5d step(s): global_step is %3d, w is %.2f,learning rate is %.2f, loss is %.2f "</span>%(i, global_step_val, w_val, learning_rate_val, loss_val))</span><br><span class="line"></span><br><span class="line"><span class="comment">#    After     0 step(s): global_step is   1, w is 3.80,learning rate is 0.10, loss is 23.04 </span></span><br><span class="line"><span class="comment">#    After     1 step(s): global_step is   2, w is 2.85,learning rate is 0.10, loss is 14.82 </span></span><br><span class="line"><span class="comment">#    ···</span></span><br><span class="line"><span class="comment">#    After    21 step(s): global_step is  22, w is -0.92,learning rate is 0.08, loss is 0.01 </span></span><br><span class="line"><span class="comment">#    After    22 step(s): global_step is  23, w is -0.94,learning rate is 0.08, loss is 0.00 </span></span><br><span class="line"><span class="comment">#    ···</span></span><br><span class="line"><span class="comment">#    After    39 step(s): global_step is  40, w is -1.00,learning rate is 0.07, loss is 0.00</span></span><br></pre></td></tr></table></figure></p><h3 id="滑动平均"><a href="#滑动平均" class="headerlink" title="滑动平均"></a>滑动平均</h3><p>记录每个参数一段时间内的过往值的平均，增加了模型的泛化性。<br>针对所有的参数: w 和 b</p><script type="math/tex; mode=display">shadow\_variable = \beta shadow\_variable + (1- \beta)variable</script><script type="math/tex; mode=display">\beta = \min \{ init\_\beta ,\ \frac{1+num\_update}{10 + num\_update} \}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ema = tf.train.ExponentialMovingAverage(moving_average_decay, global_step)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ema_op = ema.apply([])</span></span><br><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">    train_op = tf.no_op(name = <span class="string">'train'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ema.average(参数名)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># moving_average_decay 衰减率,一般比较大  init_beta</span></span><br><span class="line"><span class="comment"># global_step 当前轮数  num_update</span></span><br><span class="line"><span class="comment"># tf.trainable_variables() 所有可训练的参数</span></span><br></pre></td></tr></table></figure><p>例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义变量及滑动平均类</span></span><br><span class="line">w1 = tf.Variable(<span class="number">0</span>, dtype = tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义迭代次数</span></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>, trainable = <span class="literal">False</span>)</span><br><span class="line">moving_average_decay = <span class="number">0.99</span></span><br><span class="line">ema = tf.train.ExponentialMovingAverage(moving_average_decay, global_step)</span><br><span class="line">ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印当前w1和w1的滑动平均值</span></span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 参数w1的赋值为1</span></span><br><span class="line">    sess.run(tf.assign(w1, <span class="number">1</span>))</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 参数w1的赋值为10, global_step赋值为100</span></span><br><span class="line">    sess.run(tf.assign(global_step, <span class="number">100</span>))</span><br><span class="line">    sess.run(tf.assign(w1, <span class="number">10</span>))</span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br><span class="line"></span><br><span class="line">    sess.run(ema_op)</span><br><span class="line">    print(sess.run([w1, ema.average(w1)]))</span><br></pre></td></tr></table></figure><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p>正则化在损失函数中引入模型复杂度指标，利用<strong>给W 加权值</strong>，弱化训练数据噪声</p><script type="math/tex; mode=display">loss = loss(y, y_{-}) + regularizer * loss(w)</script><h4 id="L1-正则化"><a href="#L1-正则化" class="headerlink" title="L1 正则化"></a>L1 正则化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss(w) = tf.contrib.layers.l1_regularizer(regularizer)(w)</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">loss_{L1}(w) = \sum_{i}{|w_i|}</script><h4 id="L2-正则化"><a href="#L2-正则化" class="headerlink" title="L2 正则化"></a>L2 正则化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss(w) = tf.contrib.layers.l2_regularizer(regularizer)(w)</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">loss_{L2}(w) = \sum_{i}{|w_{i}^{2}|}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.add_to_collection(<span class="string">'losser'</span>, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line"><span class="comment"># 把内容tf.contrib.layers.l2_regularizer(regularizer)(w)  加到集合losser对应位置，做加法</span></span><br><span class="line"></span><br><span class="line">loss = cem + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br></pre></td></tr></table></figure><p>例</p><blockquote><p>数据$X[x_0, x_1]$为正态分布随机点</p><p>当$x_0^2 + x_1^2 &lt;2$时，</p><p>$\ \ \ \ y_-= 1$(红色), </p><p>$\ \ \ \ 其余y_-=0$(蓝色)</p></blockquote><p>需要用的函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.scatter(x, y, c = color)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># plt.scatter() 散点图</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">xx, yy = np.mgrid([x_start:x_end:x_step_len, y_start:y_end:y_step_len])</span><br><span class="line">grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line"><span class="comment"># ravel()   拉直</span></span><br><span class="line"><span class="comment"># np.c_()   组成矩阵</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.contour(x , y, height, levels = [line_height])</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># plt.contour()   等高线图</span></span><br><span class="line"><span class="comment"># height          点(x,y)的高度</span></span><br><span class="line"><span class="comment"># line_height     等高线的高度</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">BATCH_SIZE = <span class="number">30</span></span><br><span class="line">seed = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">rdm = np.random.RandomState(seed)</span><br><span class="line">X = rdm.randn(<span class="number">300</span>, <span class="number">2</span>)</span><br><span class="line">Y_ = [int(x0 * x0 + x1 * x1 &lt; <span class="number">2</span>) <span class="keyword">for</span> (x0, x1) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line">Y_c = [[<span class="string">'red'</span> <span class="keyword">if</span> y <span class="keyword">else</span> <span class="string">'blue'</span>] <span class="keyword">for</span> y <span class="keyword">in</span> Y_]</span><br><span class="line"></span><br><span class="line">X = np.vstack(X).reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">Y_ = np.vstack(Y_).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(X)</span><br><span class="line">print(Y_)</span><br><span class="line">print(Y_c)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c = np.squeeze(Y_c))</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    w = tf.Variable(tf.random_normal(shape), dtype = tf.float32)</span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br><span class="line">    b = tf.Variable(tf.constant(<span class="number">0.01</span>, shape = shape))</span><br><span class="line">    <span class="keyword">return</span> b</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape = (<span class="literal">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape = (<span class="literal">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1 = get_weight([<span class="number">2</span>, <span class="number">11</span>], <span class="number">0.01</span>)</span><br><span class="line">b1 = get_bias([<span class="number">11</span>])</span><br><span class="line">y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line"></span><br><span class="line">w2 = get_weight([<span class="number">11</span>, <span class="number">1</span>], <span class="number">0.01</span>)</span><br><span class="line">b2 = get_bias([<span class="number">1</span>])</span><br><span class="line">y = tf.matmul(y1, w2) + b2</span><br><span class="line"></span><br><span class="line">loss_mse = tf.reduce_mean(tf.square(y - y_))</span><br><span class="line">loss_total = loss_mse + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播，不含正则化</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_mse)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = <span class="number">40000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS): </span><br><span class="line">        start = (i * BATCH_SIZE) % <span class="number">300</span></span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict = &#123;x : X[start: end], y_ : Y_[start: end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> ==<span class="number">0</span>:</span><br><span class="line">            loss_mse_v = sess.run(loss_mse, feed_dict = &#123;x : X, y_ : Y_&#125;)</span><br><span class="line">            print(<span class="string">"After %5d step(s), loss is  %f"</span> %(i, loss_mse_v))</span><br><span class="line">    xx, yy = np.mgrid[<span class="number">-3</span>:<span class="number">3</span>:<span class="number">0.01</span>, <span class="number">-3</span>:<span class="number">3</span>:<span class="number">0.01</span>]</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    probs = sess.run(y, feed_dict = &#123;x : grid&#125;)</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line">    <span class="comment"># print("w1:\n", sess.run(w1))</span></span><br><span class="line">    <span class="comment"># print("b1:\n", sess.run(b1))</span></span><br><span class="line">    <span class="comment"># print("w2:\n", sess.run(w2))</span></span><br><span class="line">    <span class="comment"># print("b2:\n", sess.run(b2))</span></span><br><span class="line"></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c = np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx, yy, probs, levels = [<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播，含正则化</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">0.0001</span>).minimize(loss_total)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = <span class="number">40000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS): </span><br><span class="line">        start = (i * BATCH_SIZE) % <span class="number">300</span></span><br><span class="line">        end = start + BATCH_SIZE</span><br><span class="line">        sess.run(train_step, feed_dict = &#123;x : X[start: end], y_ : Y_[start: end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> ==<span class="number">0</span>:</span><br><span class="line">            loss_mse_v = sess.run(loss_mse, feed_dict = &#123;x : X, y_ : Y_&#125;)</span><br><span class="line">            print(<span class="string">"After %5d step(s), loss is %f"</span> %(i, loss_mse_v))</span><br><span class="line">    xx, yy = np.mgrid[<span class="number">-3</span>:<span class="number">3</span>:<span class="number">0.01</span>, <span class="number">-3</span>:<span class="number">3</span>:<span class="number">0.01</span>]</span><br><span class="line">    grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">    probs = sess.run(y, feed_dict = &#123;x : grid&#125;)</span><br><span class="line">    probs = probs.reshape(xx.shape)</span><br><span class="line">    <span class="comment"># print("w1:\n", sess.run(w1))</span></span><br><span class="line">    <span class="comment"># print("b1:\n", sess.run(b1))</span></span><br><span class="line">    <span class="comment"># print("w2:\n", sess.run(w2))</span></span><br><span class="line">    <span class="comment"># print("b2:\n", sess.run(b2))</span></span><br><span class="line"></span><br><span class="line">plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c = np.squeeze(Y_c))</span><br><span class="line">plt.contour(xx, yy, probs, levels = [<span class="number">.5</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/zxz007/blogimg/raw/master/img/tensenflower012.png" alt="tensenflower012" title="tensenflower012"></p><p><img src="https://gitee.com/zxz007/blogimg/raw/master/img/tensenflower013.png" alt="tensenflower013" title="tensenflower013"></p><h2 id="神经网络搭建方法"><a href="#神经网络搭建方法" class="headerlink" title="神经网络搭建方法"></a>神经网络搭建方法</h2><p>模块化搭建方法</p><h3 id="网络结构-forward-py"><a href="#网络结构-forward-py" class="headerlink" title="网络结构 forward. py"></a>网络结构 forward. py</h3><p>前向传播就是搭建网络，设计网络结构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, regularizer)</span>:</span></span><br><span class="line">    <span class="comment"># w = </span></span><br><span class="line">    <span class="comment"># b = </span></span><br><span class="line">    <span class="comment"># y = </span></span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    <span class="comment"># w = tf.Variable(tf.random_normal(shape), dtype = tf.float32)</span></span><br><span class="line">    <span class="comment"># tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))</span></span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="comment"># b = tf.Variable(tf.constant(0.01, shape = shape))</span></span><br><span class="line">    <span class="keyword">return</span> b</span><br></pre></td></tr></table></figure><h3 id="反向传播-backward-py"><a href="#反向传播-backward-py" class="headerlink" title="反向传播 backward. py"></a>反向传播 backward. py</h3><p>方向传播就是训练网络，优化网络参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bakcward</span><span class="params">()</span>:</span></span><br><span class="line">    x = tf.placeholder()</span><br><span class="line">    y_ = tf.placeholder()</span><br><span class="line">    y = forward.forward(x, REGULARIZER)</span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable = <span class="literal">False</span>)</span><br><span class="line">    loss = </span><br><span class="line">    <span class="comment"># 均方误差</span></span><br><span class="line">    loss_mse = tf.reduce_mean(tf.square(y - y_))</span><br><span class="line">    <span class="comment"># 交叉熵</span></span><br><span class="line">    ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y, labels = tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    cem = tf.reduce_mean(cd)</span><br><span class="line">    <span class="comment"># 正则化</span></span><br><span class="line">    loss = [cem | loss_mse] + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 指数衰减学习率</span></span><br><span class="line">    <span class="comment"># LREANING_RATE_STEP = 总样本数 / BATCH_SIZE</span></span><br><span class="line">    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP, LEARNING_RATE_DECAY, staircase = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    train_step = </span><br><span class="line">    <span class="comment"># train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</span></span><br><span class="line">    <span class="comment"># train_step = tf.train.MomentumOptimizer(learning_rate, momentum). minimize(loss)</span></span><br><span class="line">    <span class="comment"># train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 滑动平均</span></span><br><span class="line">    ema = tf.train.ExponentialMovingAverage(moving_average_decay, global_step)</span><br><span class="line">    ema_op = ema.apply(tf.trainable_variables())</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, ema_op]):</span><br><span class="line">        train_op = tf.no_op(name = <span class="string">'train'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            <span class="comment"># start = </span></span><br><span class="line">            <span class="comment"># end = </span></span><br><span class="line">            sess.run(train_step, feed_dict = &#123;x: ,y_: &#125;)</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">                print()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    backward()</span><br></pre></td></tr></table></figure><h3 id="模块化例子"><a href="#模块化例子" class="headerlink" title="模块化例子"></a>模块化例子</h3><h4 id="generateds-py"><a href="#generateds-py" class="headerlink" title="generateds. py"></a>generateds. py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">seed = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateds</span><span class="params">()</span>:</span></span><br><span class="line">    rdm = np.random.RandomState(seed)</span><br><span class="line">    X = rdm.randn(<span class="number">300</span>, <span class="number">2</span>)</span><br><span class="line">    Y_ = [int(x0 * x0 + x1 * x1 &lt; <span class="number">2</span>)  <span class="keyword">for</span> (x0, x1) <span class="keyword">in</span> X]</span><br><span class="line">    Y_c = [[<span class="string">'red'</span> <span class="keyword">if</span> y <span class="keyword">else</span> <span class="string">'blue'</span>] <span class="keyword">for</span> y <span class="keyword">in</span> Y_]</span><br><span class="line">    X = np.vstack(X).reshape(<span class="number">-1</span>, <span class="number">2</span>)</span><br><span class="line">    Y_ = np.vstack(Y_).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> X, Y_, Y_c</span><br></pre></td></tr></table></figure><h4 id="forward-py"><a href="#forward-py" class="headerlink" title="forward. py"></a>forward. py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_weight</span><span class="params">(shape, regularizer)</span>:</span></span><br><span class="line">    w = tf.Variable(tf.random_normal(shape), dtype = tf.float32)</span><br><span class="line">    tf.add_to_collection(<span class="string">'losses'</span>, tf.contrib.layers.l2_regularizer(regularizer)(w))</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_bias</span><span class="params">(shape)</span>:</span></span><br><span class="line">    b = tf.Variable(tf.constant(<span class="number">0.01</span>, shape = shape))</span><br><span class="line">    <span class="keyword">return</span> b</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, regularizer)</span>:</span></span><br><span class="line">    w1 = get_weight([<span class="number">2</span>, <span class="number">11</span>], regularizer)</span><br><span class="line">    b1 = get_bias([<span class="number">11</span>])</span><br><span class="line">    y1 = tf.nn.relu(tf.matmul(x, w1) + b1)</span><br><span class="line"></span><br><span class="line">    w2 = get_weight([<span class="number">11</span>, <span class="number">1</span>], regularizer)</span><br><span class="line">    b2 = get_bias([<span class="number">1</span>])</span><br><span class="line">    y = tf.matmul(y2, w2) + b2</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y</span><br></pre></td></tr></table></figure><h4 id="backward-py"><a href="#backward-py" class="headerlink" title="backward. py"></a>backward. py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> generateds</span><br><span class="line"><span class="keyword">import</span> forward</span><br><span class="line"></span><br><span class="line">STEPTS = <span class="number">40000</span></span><br><span class="line">BATCH_SIZE = <span class="number">30</span></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.001</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.999</span></span><br><span class="line">REGULARIZER = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">()</span>:</span></span><br><span class="line">    x = tf.palceholder(tf.float32, shape = (<span class="literal">None</span>, <span class="number">2</span>))</span><br><span class="line">    y_ = tf.palceholder(tf.float32, shape = (<span class="literal">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    X, Y_, Y_c = generateds.generateds()</span><br><span class="line"></span><br><span class="line">    y = forward.forward(x, REGULARIZER)</span><br><span class="line"></span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, <span class="number">300</span>/BATCH_SIZE, LEARNING_RATE_DECAY, staircase = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    loss_mse = tf.reducd_mean(tf.square(y - y_))</span><br><span class="line">    loss_total = loss_mse + tf.add_n(tf.get_collection(<span class="string">'losses'</span>))</span><br><span class="line"></span><br><span class="line">    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_total)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        init_op = tf.global_variables_initializer()</span><br><span class="line">        sess.run(init_op)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">            start = (i * BATCHSIZE) % <span class="number">300</span></span><br><span class="line">            end = start + BATCH_SIZE</span><br><span class="line">            sess.run(train_step, feed_dict = &#123;x: X[start: end], y_: Y_[start: end]&#125;)</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">2000</span> ==<span class="number">0</span>:</span><br><span class="line">                loss_v = sess.run(loss_tatal, feed_dect = &#123;x:X, y_:Y_&#125;)</span><br><span class="line">                print(<span class="string">"After %5d step(s), loss is: %f"</span>%(i, loss_v))</span><br><span class="line">        </span><br><span class="line">        xx, yy = np.mgrid[<span class="number">-3</span>:<span class="number">3</span>:<span class="number">0.01</span>, <span class="number">-3</span>:<span class="number">3</span>:<span class="number">0.01</span>]</span><br><span class="line">        grid = np.c_[xx.ravel(), yy.ravel()]</span><br><span class="line">        probs = sess.run(y, feed_dect = &#123;x:grid&#125;)</span><br><span class="line">        probs = probs.reshape(xx.shape)</span><br><span class="line"></span><br><span class="line">    plt.scatter(X[:,<span class="number">0</span>], X[:,<span class="number">1</span>], c = np.squeeze(Y_c))</span><br><span class="line">    plt.contour(xx, yy, probs, levels = [<span class="number">0.5</span>])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span></span><br><span class="line">    backward()</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note info&quot;&gt;
 神经网络优化：损失函数、学习率、滑动平均、正则化
&lt;/div&gt;
    
    </summary>
    
      <category term="Tensorflow" scheme="https://www.smartzhi.site/categories/Tensorflow/"/>
    
    
      <category term="MOOC" scheme="https://www.smartzhi.site/tags/MOOC/"/>
    
      <category term="Tensorflow" scheme="https://www.smartzhi.site/tags/Tensorflow/"/>
    
      <category term="激活函数" scheme="https://www.smartzhi.site/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
      <category term="神经网络优化" scheme="https://www.smartzhi.site/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96/"/>
    
      <category term="模块化" scheme="https://www.smartzhi.site/tags/%E6%A8%A1%E5%9D%97%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow笔记（第二讲）</title>
    <link href="https://www.smartzhi.site/2019/Tensorflow02/"/>
    <id>https://www.smartzhi.site/2019/Tensorflow02/</id>
    <published>2019-07-08T13:06:54.000Z</published>
    <updated>2020-04-26T17:09:46.730Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><div class="note info">Tensorflow框架</div><a id="more"></a><h2 id="张量、计算图、会话"><a href="#张量、计算图、会话" class="headerlink" title="张量、计算图、会话"></a>张量、计算图、会话</h2><p>基于Tensorflow的NN：用张量表示数据，用计算图搭建神经网络，用会话执行计算图，优化线上的权重（参数），得到模型。</p><h3 id="张量-tensor"><a href="#张量-tensor" class="headerlink" title="张量 (tensor)"></a>张量 (tensor)</h3><ul><li>多维数组（列表），可表示 0~n 阶数组</li><li>阶：张量的维数</li></ul><div class="table-container"><table><thead><tr><th>维数</th><th>阶</th><th>名字</th><th style="text-align:center">例子</th></tr></thead><tbody><tr><td>0-D</td><td>0</td><td>标量 scalar</td><td style="text-align:center"><code>s = 1</code></td></tr><tr><td>1-D</td><td>1</td><td>向量 vector</td><td style="text-align:center"><code>v = [1, 2, 3]</code></td></tr><tr><td>2-D</td><td>2</td><td>矩阵 matrix</td><td style="text-align:center"><code>m = [[1, 2, 3], [4, 5, 6]]</code></td></tr><tr><td>n-D</td><td>n</td><td>张量 tensor</td><td style="text-align:center">$t= \underbrace{[ [ [ }_{n个}\cdots$</td></tr></tbody></table></div><figure class="highlight python"><figcaption><span>python2 </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">b = tf.constant([<span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line">reuslt = a + b</span><br><span class="line"><span class="keyword">print</span> result </span><br><span class="line"></span><br><span class="line"><span class="comment"># Tensor("add:0", shape=(2,), dtype=float32)</span></span><br></pre></td></tr></table></figure><h3 id="计算图-graph"><a href="#计算图-graph" class="headerlink" title="计算图 (graph)"></a>计算图 (graph)</h3><ul><li>搭建神经网络的计算过程，只搭建，不运算<br><img src="https://gitee.com/zxz007/blogimg/raw/master/img/tensenflower005.jpg" alt="tensenflower005" title="tensenflower005"></li></ul><h3 id="会话-Session"><a href="#会话-Session" class="headerlink" title="会话(Session)"></a>会话(Session)</h3><ul><li>执行计算图中的节点运算</li></ul><figure class="highlight python"><figcaption><span>python2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">w = tf.constant([<span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line">y = tf.matmul(x,w)</span><br><span class="line"><span class="keyword">print</span> y</span><br><span class="line"><span class="comment"># Tensor("matmul:0", shape(1,1), dtype=float32)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">print</span> sess.run(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [[11.]]</span></span><br></pre></td></tr></table></figure><h2 id="神经网络实现过程"><a href="#神经网络实现过程" class="headerlink" title="神经网络实现过程"></a>神经网络实现过程</h2><blockquote><ol><li>准备数据集，提取特征，作为输入喂给神经网络</li><li>搭建NN结构，从输入到输出（先搭建计算图，再用会话执行）<br>（NN前向传播算法 $\Rightarrow$ 计算输出）</li><li>大量特征数据喂给NN，迭代优化NN参数<br>（NN反向传播算法 $\Rightarrow$ 优化参数训练模型）</li><li>使用训练好的模型预测和分类</li></ol></blockquote><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><ul><li>权重W, 用变量表示，随机给初值<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># W 随机生成方法</span></span><br><span class="line"></span><br><span class="line">w = tf.Variable(tf.random_normal([<span class="number">2</span>,<span class="number">3</span>], stddev = <span class="number">2</span>, mean = <span class="number">0</span>, seed = <span class="number">1</span>))</span><br><span class="line"><span class="comment"># tf.random_normal 正态分布 </span></span><br><span class="line"><span class="comment"># [2,3] 2*3矩阵</span></span><br><span class="line"><span class="comment"># stddev 标准差</span></span><br><span class="line"><span class="comment"># mean 均值</span></span><br><span class="line"><span class="comment"># seed 随机种子 随机种子相同，生成的随机数相同</span></span><br></pre></td></tr></table></figure></li></ul><div class="table-container"><table><thead><tr><th>函数</th><th>作用</th><th style="text-align:center">示例</th><th style="text-align:center">结果</th></tr></thead><tbody><tr><td><code>tf.truncated_normal()</code></td><td>去过大偏离点的正态分布</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td><code>tf.random_uniform()</code></td><td>均匀分布</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td><code>tf.zeros()</code></td><td>全0 数组</td><td style="text-align:center"><code>tf.zeros([3,2], int32)</code></td><td style="text-align:center"><code>[[0, 0], [0, 0], [0, 0]]</code></td></tr><tr><td><code>tf.ones()</code></td><td>全1 数组</td><td style="text-align:center"><code>tf.ones([3,2], int32)</code></td><td style="text-align:center"><code>[[1, 1], [1, 1], [1, 1]]</code></td></tr><tr><td><code>tf.fill()</code></td><td>全定值数组</td><td style="text-align:center"><code>tf.fill([3,2], 6)</code></td><td style="text-align:center"><code>[[6, 6], [6, 6], [6, 6]]</code></td></tr><tr><td><code>tf.constant()</code></td><td>直接给值</td><td style="text-align:center"><code>tf.constant([3, 2, 1])</code></td><td style="text-align:center"><code>[3, 2, 1]</code></td></tr></tbody></table></div><h3 id="前向传播-1"><a href="#前向传播-1" class="headerlink" title="前向传播"></a>前向传播</h3><ul><li>搭建模型，实现推理</li></ul><p><img src="https://gitee.com/zxz007/blogimg/raw/master/img/tensenflower006.jpg" alt="tensenflower006" title="tensenflower006"></p><ul><li>$X$ 是输入为 $1×2$ 矩阵</li><li>$ w^{(k)}_{i,j} $ 为待优化参数：$ i $ 为前节点编号、 $ j $ 后节点编号、 $ k $ 层数</li></ul><script type="math/tex; mode=display">W^{(1)} = \begin{bmatrix}    w^{(1)}_{1,1} & w^{(1)}_{1,2} & w^{(1)}_{1,3} \\    w^{(1)}_{2,1} & w^{(1)}_{2,2} & w^{(1)}_{2,3} \\\end{bmatrix}</script><script type="math/tex; mode=display">a^{(1)} =[a_{11}, a_{12},a_{13}] = XW^{(1)}</script><script type="math/tex; mode=display">W^{(2)} = \begin{bmatrix}    w^{(2)}_{1,1} \\    w^{(2)}_{2,1} \\    w^{(2)}_{3,1} \\\end{bmatrix}</script><script type="math/tex; mode=display">y=a^{(1)}W^{(1)}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">a = tf.matmul(X, W1)</span><br><span class="line">y = tf.matmul(a, W2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 变量初始化、计算图节点运，算都要用会话（with结构）实现</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有变量初始化：在sess.run函数中用tf.global_variables_initializer()</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算图节点运算：在sess.run函数中写入待运算的节点</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用tf.placeholder占位，在sess.run函数中用feed_dict喂数据</span></span><br><span class="line"><span class="comment"># 喂一组数据：</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape = (<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(y, feed_dic = &#123;x: [[<span class="number">0.5</span>, <span class="number">0.6</span>]]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 喂多组数据：</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape = (<span class="literal">None</span>,<span class="number">2</span>))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(y, feed_dic = &#123;x: [[<span class="number">0.1</span>, <span class="number">0.2</span>], [<span class="number">0.2</span>, <span class="number">0.3</span>], [<span class="number">0.3</span>, <span class="number">0.4</span>], [<span class="number">0.4</span>, <span class="number">0.5</span>]]&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>python2  example1</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 两层简单神经网络（全连接）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入和参数</span></span><br><span class="line">x = tf.constant([[<span class="number">0.7</span>, <span class="number">0.5</span>]])</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev = <span class="number">1</span>, seed = <span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev = <span class="number">1</span>, seed = <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    ptint <span class="string">"y is: "</span>,sess.run(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># y is [[3.0904665]]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>python2  example2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 两层简单神经网络（全连接）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入和参数</span></span><br><span class="line"><span class="comment"># 用placeholder 实现输入定义（sess.run 中喂一组数据）</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape = (<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev = <span class="number">1</span>, seed = <span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev = <span class="number">1</span>, seed = <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    ptint <span class="string">"y is: "</span>,sess.run(y, feed_dict = &#123;x: [[<span class="number">0.7</span>, <span class="number">0.5</span>]]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># y is [[3.0904665]]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>python2  example3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 两层简单神经网络（全连接）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输入和参数</span></span><br><span class="line"><span class="comment"># 用placeholder 实现输入定义（sess.run 中喂多组数据）</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape = (<span class="literal">None</span>, <span class="number">2</span>))</span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev = <span class="number">1</span>, seed = <span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev = <span class="number">1</span>, seed = <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义前向传播</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用会话计算结果</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    ptint <span class="string">"y is: \n"</span>,sess.run(y, feed_dict = &#123;x: [[<span class="number">0.7</span>, <span class="number">0.5</span>], [<span class="number">0.2</span>, <span class="number">0.3</span>], [<span class="number">0.3</span>, <span class="number">0.4</span>], [<span class="number">0.4</span>, <span class="number">0.5</span>]]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># y is </span></span><br><span class="line"><span class="comment"># [[3.0904665]</span></span><br><span class="line"><span class="comment">#  [1.2236414]</span></span><br><span class="line"><span class="comment">#  [1.72707319]</span></span><br><span class="line"><span class="comment">#  [2.23050475]]</span></span><br></pre></td></tr></table></figure><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>训练模型参数，在所有参数上用梯度下降，使NN 模型在训练数据上的损失函数最小</p><h3 id="损失函数-loss"><a href="#损失函数-loss" class="headerlink" title="损失函数(loss)"></a>损失函数(loss)</h3><p>预测值与已知答案的差距<br>均方误差MSE </p><script type="math/tex; mode=display">MSE(y\_,y) = \frac{\sum_{i=1}^n(y-y\_)^2}{n}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(tf.square(y_ - y))</span><br></pre></td></tr></table></figure><p>反向传播训练方法：以减小loss 值为优化目标<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</span><br><span class="line">train_step = tf.train.MomentumOptimizer(learning_rate, momentum). minimize(loss)</span><br><span class="line">train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)</span><br></pre></td></tr></table></figure></p><p>学习率：决定参数每次更新幅度 （小一点 0.001）</p><figure class="highlight python"><figcaption><span>python2  example4</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="comment"># 0 导入模块，生成模拟数据</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">BATCH_SIZE = <span class="number">8</span></span><br><span class="line">seed = <span class="number">23455</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于seed 产生随机数</span></span><br><span class="line">rng = np.random.RandomState(seed)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入数据集：随机数返回32行2列的矩阵  表示32组 体重和重量</span></span><br><span class="line">X = rng.rand(<span class="number">32</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入数据标签：从X取出一行，如果和小于1，给Y赋值1； 否则Y赋值0</span></span><br><span class="line">Y = [[int(x0 + x1 &lt; <span class="number">1</span>)] <span class="keyword">for</span> (x0, x1) <span class="keyword">in</span> X]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 定义神经网络的输入、参数和输出，定义前向传播过程</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape = (<span class="literal">None</span>, <span class="number">2</span>))</span><br><span class="line">y_ = tf.placeholder(tf.float32, shape = (<span class="literal">None</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev = <span class="number">1</span>, seed = <span class="number">1</span>))</span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev = <span class="number">1</span>, seed = <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2 定义损失函数及反向传播方法</span></span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_))</span><br><span class="line">train_stpe = tf.train.GradientDescentOptimizer(<span class="number">0.001</span>).minimize(loss)</span><br><span class="line"><span class="comment"># train_step = tf.train.MomentumOptimizer(0.001, 0.9). minimize(loss)</span></span><br><span class="line"><span class="comment"># train_step = tf.train.AdamOptimizer(0.001).minimize(loss)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3 生成会话，训练STEPS轮</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="comment"># 输出未训练的参数值</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w1:\n"</span>, sess.run(w1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w2:\n"</span>, sess.run(w2)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"\n"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    STEPS = <span class="number">3000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = (i * BATCH_SIZE) % <span class="number">32</span></span><br><span class="line">        end = start + BATCH_SIZE   </span><br><span class="line">        sess.run(train_step, feed_dict = &#123;x: X[start: end], y_: Y[start, end]&#125;)</span><br><span class="line">        <span class="keyword">if</span> i  % <span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line">            total_loss = sess.run(loss, feed_dict = &#123;x: X, y_: Y&#125;)</span><br><span class="line">            print(<span class="string">"After %d training stap(s), loss on all data is %g"</span> % (i, total_loss))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"\n"</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w1:\n"</span>, sess.run(w1)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"w2:\n"</span>, sess.run(w2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># w1:</span></span><br><span class="line"><span class="comment"># [[-0.81131822   1.48459876  0.06532937]</span></span><br><span class="line"><span class="comment">#  [-2.4427042    0.0992484   0.59122431]]</span></span><br><span class="line"><span class="comment"># w2:</span></span><br><span class="line"><span class="comment"># [[-0.81131822]</span></span><br><span class="line"><span class="comment">#  [ 1.48459876]</span></span><br><span class="line"><span class="comment">#  [ 0.06532937]]</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># After 0 training step(s), loss on all data is 5.13118</span></span><br><span class="line"><span class="comment"># After 500 training step(s), loss on all data is 0.429111</span></span><br><span class="line"><span class="comment"># After 1000 training step(s), loss on all data is 0.409789</span></span><br><span class="line"><span class="comment"># After 1500 training step(s), loss on all data is 0.399923</span></span><br><span class="line"><span class="comment"># After 2000 training step(s), loss on all data is 0.394146</span></span><br><span class="line"><span class="comment"># After 2500 training step(s), loss on all data is 0.390597</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># w1:</span></span><br><span class="line"><span class="comment"># [[-0.70006633   0.9136318   0.08953571]</span></span><br><span class="line"><span class="comment">#  [-2.3402493    -0.14641267 0.58823055]]</span></span><br><span class="line"><span class="comment"># w2:</span></span><br><span class="line"><span class="comment"># [[-0.06024267]</span></span><br><span class="line"><span class="comment">#  [ 0.91956186]</span></span><br><span class="line"><span class="comment">#  [-0.0682071 ]]</span></span><br></pre></td></tr></table></figure><h2 id="搭建神经网络步骤"><a href="#搭建神经网络步骤" class="headerlink" title="搭建神经网络步骤"></a>搭建神经网络步骤</h2><p>1 准备</p><blockquote><p>import<br>常量定义<br>生成数据集</p></blockquote><p>2 前向传播：定义输入、参数和输出</p><blockquote><p>x  =<br>y_ =</p><p>w1 =<br>w2 =</p><p>a =<br>y = </p></blockquote><p>3 反向传播：定义损失函数、反向传播方法</p><blockquote><p>loss =<br>train_step = </p></blockquote><p>4 生成会话，训练STEPS轮<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    STEPS = </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(STEPS):</span><br><span class="line">        start = </span><br><span class="line">        end = </span><br><span class="line">        sess.run(train_step, feed_dict)</span><br></pre></td></tr></table></figure></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note info&quot;&gt;
Tensorflow框架
&lt;/div&gt;
    
    </summary>
    
      <category term="Tensorflow" scheme="https://www.smartzhi.site/categories/Tensorflow/"/>
    
    
      <category term="MOOC" scheme="https://www.smartzhi.site/tags/MOOC/"/>
    
      <category term="Tensorflow" scheme="https://www.smartzhi.site/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow笔记（第一讲）</title>
    <link href="https://www.smartzhi.site/2019/Tensorflow01/"/>
    <id>https://www.smartzhi.site/2019/Tensorflow01/</id>
    <published>2019-07-08T12:31:56.000Z</published>
    <updated>2020-04-26T17:11:14.850Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p class="description">人工智能概述</p><a id="more"></a><h2 id="人工智能-vs-机器学习-vs-深度学习"><a href="#人工智能-vs-机器学习-vs-深度学习" class="headerlink" title="人工智能 vs 机器学习 vs 深度学习"></a>人工智能 vs 机器学习 vs 深度学习</h2><p>人工智能，机器模拟人的意识和思维；<br>机器学习，实现人工智能的一种方法，是人工智能的子集。<br>深度学习，是深层次神经网络，是机器学习的一种实现方法，是机器学习的子集。</p><p><img src="https://gitee.com/zxz007/blogimg/raw/master/img/tensenflower002.jpg" alt="tensenflower002" title="tensenflower002"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>机器学习的定义：如果一个程序可在任务T上，随经验E的增加，效果P随之增加，则这个程序可以从经验中学习。<br><img src="https://gitee.com/zxz007/blogimg/raw/master/img/tensenflower003.jpg" alt="tensenflower003" title="tensenflower003"></p><p><img src="https://gitee.com/zxz007/blogimg/raw/master/img/tensenflower004.jpg" alt="tensenflower004" title="tensenflower004"></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;p class=&quot;description&quot;&gt;人工智能概述&lt;/p&gt;
    
    </summary>
    
      <category term="Tensorflow" scheme="https://www.smartzhi.site/categories/Tensorflow/"/>
    
    
      <category term="MOOC" scheme="https://www.smartzhi.site/tags/MOOC/"/>
    
      <category term="Tensorflow" scheme="https://www.smartzhi.site/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>边缘检测</title>
    <link href="https://www.smartzhi.site/2019/CNN01/"/>
    <id>https://www.smartzhi.site/2019/CNN01/</id>
    <published>2019-04-19T12:48:42.000Z</published>
    <updated>2019-04-23T09:16:04.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p class="description"></p><a id="more"></a><h2 id="卷积计算方法"><a href="#卷积计算方法" class="headerlink" title="卷积计算方法"></a>卷积计算方法</h2><h3 id="卷积计算方法示意"><a href="#卷积计算方法示意" class="headerlink" title="卷积计算方法示意"></a>卷积计算方法示意</h3><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g288zo2mgyg30hs0cw0vc.gif" alt="cnn001_001" title="卷积计算示意图"></p><h3 id="卷积计算示例"><a href="#卷积计算示例" class="headerlink" title="卷积计算示例"></a>卷积计算示例</h3><script type="math/tex; mode=display">\begin{bmatrix}3&3&2&1&0\\0&0&1&3&1\\3&1&2&2&3\\2&0&0&2&2\\2&0&0&0&1\\\end{bmatrix} *\begin{bmatrix}0&1&2\\2&2&0\\0&1&2\\\end{bmatrix} =\begin{bmatrix}12&12&17\\10&17&19\\9&6&14\\\end{bmatrix}</script><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g2895dhdllg30ev08b0x5.gif" alt="cnn001_002" title="卷积计算示例"></p><h2 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h2><h3 id="垂直边缘检测"><a href="#垂直边缘检测" class="headerlink" title="垂直边缘检测"></a>垂直边缘检测</h3><script type="math/tex; mode=display">\begin{bmatrix}10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\10&10&10&0&0&0\\\end{bmatrix} *\begin{bmatrix}1&0&-1\\1&0&-1\\1&0&-1\\\end{bmatrix} =\begin{bmatrix}0&30&30&0\\0&30&30&0\\0&30&30&0\\0&30&30&0\\\end{bmatrix}</script><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g2cd536jdlj30gt04ia9w.jpg" alt="cnn001_003" title="边缘检测示意图"></p><h3 id="边缘过渡"><a href="#边缘过渡" class="headerlink" title="边缘过渡"></a>边缘过渡</h3><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><blockquote><p>取绝对值可以忽略过度<br>不取绝对值，可以得到过度信息</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g2ckkpd2i8j30mi0lk0u1.jpg" alt="cnn001_004" title="边缘过度"></p><h4 id="水平检测"><a href="#水平检测" class="headerlink" title="水平检测"></a>水平检测</h4><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g2clajy42zj30fc05vdhb.jpg" alt="cnn001_005" title="水平边缘检测"></p><h3 id="滤波器种类"><a href="#滤波器种类" class="headerlink" title="滤波器种类"></a>滤波器种类</h3><script type="math/tex; mode=display">\begin{bmatrix}1&0&-1\\2&0&-2\\1&0&-1\\\end{bmatrix}\ \ \ \ \ \ \  \begin{bmatrix}3&0&-3\\10&0&-10\\3&0&-3\\\end{bmatrix}\ \ \ \ \ \ \  \begin{bmatrix}\omega _1&\omega _2&\omega _3\\\omega _4&\omega _5&\omega _6\\\omega _7&\omega _8&\omega _9\\\end{bmatrix}</script><center><b>sobe fitter &emsp; &emsp; &emsp; &emsp;  scharr fitter  &emsp; &emsp; &emsp; &emsp;  训练滤波器</b></center><hr>]]></content>
    
    <summary type="html">
    
      &lt;p class=&quot;description&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="CNN" scheme="https://www.smartzhi.site/categories/CNN/"/>
    
    
      <category term="图像" scheme="https://www.smartzhi.site/tags/%E5%9B%BE%E5%83%8F/"/>
    
      <category term="边缘检测" scheme="https://www.smartzhi.site/tags/%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"/>
    
      <category term="卷积" scheme="https://www.smartzhi.site/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>MySQL 基础</title>
    <link href="https://www.smartzhi.site/2019/mysql002/"/>
    <id>https://www.smartzhi.site/2019/mysql002/</id>
    <published>2019-04-02T13:43:57.000Z</published>
    <updated>2019-06-11T09:15:26.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><div class="note info">登陆、启动 MySQL</div><br><a id="more"></a></p><h2 id="登陆-MySQL-数据库"><a href="#登陆-MySQL-数据库" class="headerlink" title="登陆 MySQL 数据库"></a>登陆 MySQL 数据库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -h hostname -u username -p</span><br></pre></td></tr></table></figure><p>在上述命令中，mysql为登录命令，-h后面的参数是服务器的主机地址，-u后面的参数是登录数据库的用户名，-p后面是登录密码</p><h2 id="启动-MySQL-服务"><a href="#启动-MySQL-服务" class="headerlink" title="启动 MySQL 服务"></a>启动 MySQL 服务</h2><figure class="highlight bash"><figcaption><span>启动 MySQL 服务</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net start mysql</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">⋈ ⋉ ⋊</script><figure class="highlight bash"><figcaption><span>停止 MySQL 服务</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net stop mysql</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;div class=&quot;note info&quot;&gt;
登陆、启动 MySQL
&lt;/div&gt;&lt;br&gt;
    
    </summary>
    
      <category term="数据库" scheme="https://www.smartzhi.site/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://www.smartzhi.site/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="MySQL" scheme="https://www.smartzhi.site/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>数据库入门</title>
    <link href="https://www.smartzhi.site/2019/mysql001/"/>
    <id>https://www.smartzhi.site/2019/mysql001/</id>
    <published>2019-04-02T13:07:13.000Z</published>
    <updated>2019-09-17T07:21:04.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><div class="note info">数据库基本概念、数据库模型概念、关系数据库</div><a id="more"></a><h2 id="DBMS"><a href="#DBMS" class="headerlink" title="DBMS"></a>DBMS</h2><ol><li>数据定义功能</li><li>数据操纵功能</li><li>数据库的运行管理功能</li><li>数据组织、存储与管理功能</li><li>数据库的保护功能</li><li>数据库的维护功能</li><li>数据库接口功能</li></ol><h2 id="数据库模型概念"><a href="#数据库模型概念" class="headerlink" title="数据库模型概念"></a>数据库模型概念</h2><p>数据模型（Data Model）它是数据特征的抽象。数据模型是数据库系统的核心与基础，它从抽象层次上描述了系统的静态特征、动态行为和约束条件，为数据库系统的信息表示与操作提供了一个抽象的框架。</p><ol><li>数据结构：主要描述数据的类型、内容、性质以及数据间的联系等，是对系统静态特征的描述。</li><li>数据操作：主要描述在相应的数据结构上的操作类型和操作方式，是对系统动态特征的描述。</li><li>数据的约束条件：主要描述数据结构内数据间的语法、词义联系、他们之间的制约和依存关系，以及数据动态变化的规则，以保证数据的正确、有效和相容。</li></ol><blockquote><p>概念模型表示方法<br>概念模型的表示方法有很多，但最常用的方法为实体-联系方法（Entity-Relationship Approach），简称E-R方法<br>该方法用E-R图（Entity-Relationship Diagram，实体-联系图）来描述现实世界的概念模型，E-R方法也称为E-R模型（Entity-Relationship Model）。</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g1olv8j6zrj30is075t8u.jpg" alt></p><blockquote><p>实体之间的联系：一对一、一对多、多对多</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g1olxk5m4aj30l1079weq.jpg" alt></p><blockquote><p>如果联系也具有属性，则这些属性也要用无向边与该联系连接起来例如学生与课程之间存在学习的联系，学习就有“成绩”这一属性。</p></blockquote><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g1om088y3dj30br09cwem.jpg" alt></p><h2 id="关系数据库规范化"><a href="#关系数据库规范化" class="headerlink" title="关系数据库规范化"></a>关系数据库规范化</h2><p>范式是符合某一种级别的关系模式的集合，是衡量关系模式规范化程度的标准，符合标准的关系才是规范化的。范式可以分为多个等级：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、BC范式（BCNF）、第四范式等。</p><h3 id="第一范式"><a href="#第一范式" class="headerlink" title="第一范式"></a>第一范式</h3><p>如果关系模式R中所有的属性都是不可分解的，则称该关系模式R满足第一范式（First Normal Form），简称1NF，记作R$\in$1NF。</p><h3 id="第二范式"><a href="#第二范式" class="headerlink" title="第二范式"></a>第二范式</h3><p>如果一个关系模式R$\in$1NF，且R中的每一个非主属性都完全丽数依赖于码，则称该关系模式R满足第二范式（Second Normal Form），简称2NF，记作R$\in$2NF。</p><blockquote><p>例如，学生成绩表（学号，课程号，姓名，课程名，成绩）中，<br>“学号”和“课程号”字段组成主键，“成绩”完全依赖于该主键，但是“姓名”和“课程名”都只是部分依赖于主键，<br>“姓名”可以由“学号”确定，并不需要“课程号”，而“课程名”是由“课程号”决定并不依赖于“学号”。所以该关系表就不符合2NF。</p><p>对于上面的这种关系，可以将其分解为三张表：<br>(1) 学生信息表（学号，姓名）<br>(2) 课程信息表（课程号，课程名）<br>(3) 成绩表（学号，课程号，成绩）</p></blockquote><h3 id="第三范式"><a href="#第三范式" class="headerlink" title="第三范式"></a>第三范式</h3><p>如果一个关系模式R$\in$2NF，且R中的每个非主属性都不传递函数依赖于码，则称该关系模式R满足第三范式（Third Normal Form），简称3NF，记作R$\in$3NF。<br>可以证明，若R$\in$3NF，则每一个非主属性既不部分函数依赖于码，也不传递依赖于码。</p><blockquote><p>所谓传递函数依赖是指：<br>在一个数据表中存在关键字段A决定非关键字段B，<br>而B又决定非关键字段C，则称C传递函数依赖于A，并称该表中存在传递依赖关系。</p></blockquote><h2 id="数据库设计"><a href="#数据库设计" class="headerlink" title="数据库设计"></a>数据库设计</h2><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g1omg35dt4j30qa0dqq3y.jpg" alt></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;note info&quot;&gt;
数据库基本概念、数据库模型概念、关系数据库
&lt;/div&gt;
    
    </summary>
    
      <category term="数据库" scheme="https://www.smartzhi.site/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="https://www.smartzhi.site/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="MySQL" scheme="https://www.smartzhi.site/tags/MySQL/"/>
    
      <category term="范式" scheme="https://www.smartzhi.site/tags/%E8%8C%83%E5%BC%8F/"/>
    
      <category term="关系数据库" scheme="https://www.smartzhi.site/tags/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>linux02</title>
    <link href="https://www.smartzhi.site/2019/linux02/"/>
    <id>https://www.smartzhi.site/2019/linux02/</id>
    <published>2019-03-22T03:24:54.000Z</published>
    <updated>2019-04-04T12:32:12.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><div class="note info">正则表达式</div><br><a id="more"></a></p>    <div id="aplayer-kFtynRPj" class="aplayer aplayer-tag-marker meting-tag-marker" data-id="472650231" data-server="netease" data-type="playlist" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#ad7a86"></div><h2 id="元字符"><a href="#元字符" class="headerlink" title="元字符"></a>元字符</h2><ul><li>6个元字符 <code>.</code> <code>*</code> <code>[</code> <code>\</code> <code>^</code> <code>$</code> </li><li>其他字符与其自身匹配</li><li>转义 用 <code>\</code> 转义元字符</li></ul><h2 id="单字符正则表达式"><a href="#单字符正则表达式" class="headerlink" title="单字符正则表达式"></a>单字符正则表达式</h2><ul><li>转义字符 <code>\.</code> <code>\*</code> <code>\[</code> <code>\\</code> <code>\^</code> <code>\$</code></li><li>原点 <code>.</code> 匹配任何一个字符</li><li></li></ul><h2 id><a href="#" class="headerlink" title=" "></a> </h2><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;div class=&quot;note info&quot;&gt;
正则表达式
&lt;/div&gt;&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Linux 文本文件处理</title>
    <link href="https://www.smartzhi.site/2019/linux01/"/>
    <id>https://www.smartzhi.site/2019/linux01/</id>
    <published>2019-03-21T07:50:27.000Z</published>
    <updated>2020-04-26T17:16:07.649Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p><div class="note info">Linux 文本文件处理</div><br><a id="more"></a></p><h2 id="more-less-逐屏显示文件"><a href="#more-less-逐屏显示文件" class="headerlink" title="more/less 逐屏显示文件"></a><code>more</code>/<code>less</code> 逐屏显示文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">more test.txt  <span class="comment"># one file</span></span><br><span class="line">more *.txt     <span class="comment"># many txt file </span></span><br><span class="line">ls -l | more   <span class="comment"># no file</span></span><br><span class="line">less test.txt</span><br></pre></td></tr></table></figure><h3 id="more"><a href="#more" class="headerlink" title="more"></a>more</h3><div class="table-container"><table><thead><tr><th>输入</th><th>动作</th></tr></thead><tbody><tr><td>空格</td><td>下一屏</td></tr><tr><td>回车</td><td>上滚一行</td></tr><tr><td><code>q</code></td><td>退出</td></tr><tr><td><code>/pattren</code></td><td>搜索指定模式的字符串，模式描述用正则表达式</td></tr><tr><td><code>/</code></td><td>继续查找指定模式的字符串</td></tr><tr><td><code>h</code></td><td>帮助信息</td></tr><tr><td><code>Ctrl-L</code></td><td>屏幕刷新</td></tr></tbody></table></div><h3 id="less"><a href="#less" class="headerlink" title="less"></a>less</h3><pre><code>- 回退浏览的功能更强- 可直接使键盘的上下箭头键，或者j,k，类似vi的光标定位键，以及PgUp， PgDn，Home，End键</code></pre><h2 id="cat-od-列出文件内容"><a href="#cat-od-列出文件内容" class="headerlink" title="cat/od 列出文件内容"></a><code>cat</code>/<code>od</code> 列出文件内容</h2><blockquote><p>cat  concatenate:串结，文本格式打印 （选项-n：行号）<br>od   octal dump逐字节打印（-c, -t c, -t x1，-t d1, -t u1选项）</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat -n 20 test.txt</span><br><span class="line">cat &gt;test1.txt     <span class="comment"># 从stdin获取数据，直到ctrl-d</span></span><br><span class="line"></span><br><span class="line">od -t x1 a.dat     <span class="comment"># 十六进制打印</span></span><br><span class="line">od -t xi a.dat | more</span><br><span class="line">od -c b.file       <span class="comment"># 逐字打印，遇到不可打印字符，打印编码</span></span><br></pre></td></tr></table></figure><h2 id="head-tail-显示文件的头部或者尾部"><a href="#head-tail-显示文件的头部或者尾部" class="headerlink" title="head/tail 显示文件的头部或者尾部"></a><code>head</code>/<code>tail</code> 显示文件的头部或者尾部</h2><blockquote><p>默认只选择10行，-n选项可以选择行数</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">head -n 15 test.txt</span><br><span class="line">head -n 15 test1.txt test2.txt |more  <span class="comment"># 显示2个文件各自前15行，共30行</span></span><br><span class="line">tail -n 15 test.txt</span><br><span class="line"></span><br><span class="line">head -n -20 test.txt  <span class="comment"># 去除文件尾部20行，其余算头</span></span><br><span class="line">tail -n +20 test.txt  <span class="comment"># 去除文件头部20行，其余算尾</span></span><br><span class="line"></span><br><span class="line">tail -f debug.txt     <span class="comment"># 实时打印文件尾部被追加的内容</span></span><br></pre></td></tr></table></figure><h2 id="tee-三通"><a href="#tee-三通" class="headerlink" title="tee 三通"></a><code>tee</code> 三通</h2><blockquote><p>将从标准输入stdin得到的数据抄送到标准输出stdout显示，同时存入磁盘文件中</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat a.sh</span><br><span class="line"><span class="built_in">echo</span> a</span><br><span class="line"><span class="built_in">echo</span> b</span><br><span class="line"></span><br><span class="line">sh a.sh | tee a.log</span><br><span class="line">a</span><br><span class="line">b</span><br><span class="line"></span><br><span class="line">cat a.log</span><br><span class="line">a</span><br><span class="line">b</span><br></pre></td></tr></table></figure><h2 id="wc-字数统计"><a href="#wc-字数统计" class="headerlink" title="wc 字数统计"></a><code>wc</code> 字数统计</h2><blockquote><p>列出文件中一共有多少行，有多少个单词，多少字符<br>当指定的文件数大于1时，最后还列出一个合计<br>常用选项-l：只列出行计数</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">wc test.txt             <span class="comment"># 1 个文件</span></span><br><span class="line"></span><br><span class="line">wc test1.txt test2.txt  <span class="comment"># 多个文件</span></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># 6  7 13 test1.txt</span></span><br><span class="line"><span class="comment"># 6  7 13 test2.txt</span></span><br><span class="line"><span class="comment"># 12 14 26 total</span></span><br><span class="line"></span><br><span class="line">wc -l test1.txt test2.txt</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># 6  test1.txt</span></span><br><span class="line"><span class="comment"># 6  test2.txt</span></span><br><span class="line"><span class="comment"># 12 total</span></span><br></pre></td></tr></table></figure><h2 id="sort-对文件内容排序"><a href="#sort-对文件内容排序" class="headerlink" title="sort 对文件内容排序"></a><code>sort</code> 对文件内容排序</h2><blockquote><p>-n选项(Numberic):对于数字按照算术值大小排序，而不是按照字符串比较 规则，例如123与67<br>可以选择行中某一部分作为排序关键字<br>选择升序或降序<br>字符串比较时对字母是否区分大小写<br>内排序外排序等算法参数选择（当数据量较大时，性能调优）</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sort text1.txt &gt; text3.txt</span><br></pre></td></tr></table></figure><h2 id="tr-翻译字符"><a href="#tr-翻译字符" class="headerlink" title="tr 翻译字符"></a><code>tr</code> 翻译字符</h2><blockquote><p><code>tr string1 string2</code><br>把标准输入拷贝到标准输出，string1中出现的字符替换为string2中的对应字符</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat test1.txt</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># a</span></span><br><span class="line"><span class="comment"># b</span></span><br><span class="line"><span class="comment"># c</span></span><br><span class="line"><span class="comment"># d</span></span><br><span class="line"></span><br><span class="line">cat test1.txt |tr <span class="string">'[abc]'</span> <span class="string">'[XYZ]'</span></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># X</span></span><br><span class="line"><span class="comment"># Y</span></span><br><span class="line"><span class="comment"># Z</span></span><br><span class="line"><span class="comment"># d</span></span><br></pre></td></tr></table></figure><h2 id="nuiq-筛选文件中重复行"><a href="#nuiq-筛选文件中重复行" class="headerlink" title="nuiq 筛选文件中重复行"></a><code>nuiq</code> 筛选文件中重复行</h2><p> <code>uniq options</code><br> <code>uniq options input-file</code><br> <code>uniq options input-file output-file</code></p><blockquote><p>重复的行：<strong>紧邻的两行内容相同</strong><br>选项<br>-u （uniqe）只保留没有重复的行<br>-d （duplicated）只保留有重复的行（但只打印一次） 没有以上两个选项，打印没有重复的行和有重复的行（但只打印一次)<br>-c （count）计数同样的行出现几次</p></blockquote><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;div class=&quot;note info&quot;&gt;
Linux 文本文件处理
&lt;/div&gt;&lt;br&gt;
    
    </summary>
    
      <category term="linux" scheme="https://www.smartzhi.site/categories/linux/"/>
    
    
      <category term="Linux 学习" scheme="https://www.smartzhi.site/tags/Linux-%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="文本文件处理" scheme="https://www.smartzhi.site/tags/%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Markdown写作模板</title>
    <link href="https://www.smartzhi.site/2019/markdown01/"/>
    <id>https://www.smartzhi.site/2019/markdown01/</id>
    <published>2019-03-14T07:22:19.000Z</published>
    <updated>2019-03-20T07:43:20.000Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>整理的markdown语法，以及在hexo中的特殊渲染<br><a id="more"></a></p><h2 id="markdown-在hexo-中的渲染-1"><a href="#markdown-在hexo-中的渲染-1" class="headerlink" title="markdown 在hexo 中的渲染 1"></a>markdown 在hexo 中的渲染 <sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup></h2><h3 id="主题自带样式-文本居中引用"><a href="#主题自带样式-文本居中引用" class="headerlink" title="主题自带样式 文本居中引用"></a>主题自带样式 文本居中引用</h3><p>效果：<br><blockquote class="blockquote-center"><p>人生乃是一面镜子，<br>从镜子里认识自己，<br>我要称之为头等大事，<br>也只是我们追求的目的！</p></blockquote></p><p>代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;% cq %&#125;</span><br><span class="line">人生乃是一面镜子，</span><br><span class="line">从镜子里认识自己，</span><br><span class="line">我要称之为头等大事， </span><br><span class="line">也只是我们追求的目的！</span><br><span class="line">&#123;% endcq %&#125;</span><br></pre></td></tr></table></figure></p><h3 id="主题自带样式-note-标签"><a href="#主题自带样式-note-标签" class="headerlink" title="主题自带样式 note 标签"></a>主题自带样式 note 标签</h3><div class="note default"><p>default</p></div><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="note default"&gt;&lt;p&gt;default&lt;/p&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure><p><br></p><div class="note primary"><p>primary</p></div><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="note primary"&gt;&lt;p&gt;primary&lt;/p&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure><p><br></p><div class="note success"><p>success</p></div><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="note success"&gt;&lt;p&gt;success&lt;/p&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure><p><br></p><div class="note info"><p>info</p></div><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="note info"&gt;&lt;p&gt;info&lt;/p&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure><p><br></p><div class="note warning"><p>warning</p></div><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="note warning"&gt;&lt;p&gt;warning&lt;/p&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure><p><br></p><div class="note danger"><p>danger</p></div><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="note danger"&gt;&lt;p&gt;danger&lt;/p&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure><p><br></p><div class="note danger no-icon"><p>danger no-icon</p></div><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class="note danger no-icon"&gt;&lt;p&gt;danger no-icon&lt;/p&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure><h3 id="主题自带样式-label-标签"><a href="#主题自带样式-label-标签" class="headerlink" title="主题自带样式 label 标签"></a>主题自带样式 label 标签</h3><ul><li><span class="label default">default</span></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% label default@default %&#125;</span><br></pre></td></tr></table></figure><p><br></p><ul><li><span class="label primary">primary</span></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% label primary@primary %&#125;</span><br></pre></td></tr></table></figure><p><br></p><ul><li><span class="label success">success</span></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% label success@success %&#125;</span><br></pre></td></tr></table></figure><p><br></p><ul><li><span class="label info">info</span></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% label info@info %&#125;</span><br></pre></td></tr></table></figure><p><br></p><ul><li><span class="label warning">warning</span></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% label warning@warning %&#125;</span><br></pre></td></tr></table></figure><p><br></p><ul><li><span class="label danger">danger</span></li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% label danger@danger %&#125;</span><br></pre></td></tr></table></figure><h3 id="主题自带样式-tabs-标签"><a href="#主题自带样式-tabs-标签" class="headerlink" title="主题自带样式 tabs 标签"></a>主题自带样式 tabs 标签</h3><p>效果<br><div class="tabs" id="选项卡"><ul class="nav-tabs"><li class="tab"><a href="#选项卡-1">选项卡 1</a></li><li class="tab active"><a href="#选项卡-2">选项卡 2</a></li><li class="tab"><a href="#选项卡-3">选项卡 3</a></li></ul><div class="tab-content"><div class="tab-pane" id="选项卡-1"><p><strong>这是选项卡 1</strong> 呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈……</p></div><div class="tab-pane active" id="选项卡-2"><p><strong>这是选项卡 2</strong></p></div><div class="tab-pane" id="选项卡-3"><p><strong>这是选项卡 3</strong> 哇，你找到我了！φ(≧ω≦*)♪～</p></div></div></div></p><p>源码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;% tabs 选项卡, 2 %&#125;</span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line">**这是选项卡 1** 呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈呵呵哈哈哈哈哈哈哈哈……</span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line">**这是选项卡 2**</span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&lt;!-- tab --&gt;</span><br><span class="line">**这是选项卡 3** 哇，你找到我了！φ(≧ω≦*)♪～</span><br><span class="line">&lt;!-- endtab --&gt;</span><br><span class="line">&#123;% endtabs %&#125;</span><br></pre></td></tr></table></figure><h3 id="主题自带样式-按钮"><a href="#主题自带样式-按钮" class="headerlink" title="主题自带样式 按钮"></a>主题自带样式 按钮</h3><p>源码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% btn https://www.baidu.com, 点击下载百度, download fa-lg fa-fw %&#125;</span><br></pre></td></tr></table></figure><p>效果<br><a class="btn" href="https://www.baidu.com" target="_blank" rel="noopener"><i class="fa fa-download fa-lg fa-fw"></i>点击下载百度</a></p><h3 id="自定义样式-引用"><a href="#自定义样式-引用" class="headerlink" title="自定义样式 引用"></a>自定义样式 引用</h3><div class="note info"><p>首先由于是自定义的样式，故要自己将 CSS 代码加到custom.styl中，下文的自定义样式都是如此。为什么可以自定义呢？如果你是一个和我一样的小白，可以点击这里了解了解 CSS 中id和class的知识。</p></div><p>需加入custom.styl的代码：<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// 自定义的引用样式</span><br><span class="line"><span class="selector-tag">blockquote</span><span class="selector-class">.question</span> &#123;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#555</span>;</span><br><span class="line">    <span class="attribute">border-left</span>: <span class="number">4px</span> solid <span class="built_in">rgb</span>(16, 152, 173);</span><br><span class="line">    <span class="attribute">background-color</span>: <span class="built_in">rgb</span>(227, 242, 253);</span><br><span class="line">    <span class="attribute">border-top-right-radius</span>: <span class="number">3px</span>;</span><br><span class="line">    <span class="attribute">border-bottom-right-radius</span>: <span class="number">3px</span>;</span><br><span class="line">    <span class="attribute">margin-bottom</span>: <span class="number">20px</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><ul><li>文字颜色改color的值</li><li>背景色改background-color的值</li><li>边框颜色和粗细改border-left的值</li></ul><h2 id="markdown-格式"><a href="#markdown-格式" class="headerlink" title="markdown 格式"></a>markdown 格式</h2><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 一级标题</span><br><span class="line">## 二级标题</span><br><span class="line">...</span><br><span class="line">###### 六级标题</span><br></pre></td></tr></table></figure><p>效果：<br><img src="http://ww1.sinaimg.cn/large/0065saiygy1g1799tc2g8j30gb04mmx0.jpg" alt></p><h3 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">**粗体** </span><br><span class="line">*斜体*  </span><br><span class="line">***斜体加粗*** </span><br><span class="line">~~删除线~~ </span><br><span class="line">&lt;font color =&quot;red&quot;&gt;示例md代码:&lt;/font&gt;</span><br></pre></td></tr></table></figure><ul><li><strong>粗体</strong> </li><li><em>斜体</em>  </li><li><strong><em>斜体加粗</em></strong> </li><li><del>删除线</del> </li><li><font color="red">示例md代码:</font></li></ul><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; 引用1</span><br><span class="line">&gt;&gt; 引用2</span><br></pre></td></tr></table></figure><p>效果：</p><blockquote><p>引用1</p><blockquote><p>引用2</p></blockquote></blockquote><h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">---</span><br><span class="line">***</span><br><span class="line">***</span><br></pre></td></tr></table></figure><hr><hr><h3 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h3><p>语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">![alt](url &apos;&apos;title&apos;&apos;)</span><br><span class="line"></span><br><span class="line">alt表示图片显示失败时的替换文本</span><br><span class="line">title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加</span><br><span class="line"></span><br><span class="line">例如：</span><br><span class="line">![blockchain](https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=702257389,1274025419&amp;fm=27&amp;gp=0.jpg &quot;区块链&quot;)</span><br></pre></td></tr></table></figure></p><p><img src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=702257389,1274025419&amp;fm=27&amp;gp=0.jpg" alt="pic1" title="区块链"></p><h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><p>语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[超链接名](超链接地址 &quot;超链接title&quot;)</span><br><span class="line"></span><br><span class="line">title可加可不加</span><br><span class="line"></span><br><span class="line">例如：</span><br><span class="line">[简书](http://jianshu.com)</span><br><span class="line">[百度](http://baidu.com &quot;到百度&quot;)</span><br></pre></td></tr></table></figure></p><p><a href="http://jianshu.com" target="_blank" rel="noopener">简书</a><br><a href="http://baidu.com" title="到百度" target="_blank" rel="noopener">百度</a></p><p>html语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;a href=&quot;超链接地址&quot; target=&quot;_blank&quot;&gt;超链接名&lt;/a&gt;</span><br><span class="line"></span><br><span class="line">例如：</span><br><span class="line">&lt;a href=&quot;https://www.jianshu.com&quot; target=&quot;_blank&quot;&gt;简书&lt;/a&gt;</span><br></pre></td></tr></table></figure></p><p><a href="https://www.jianshu.com" target="_blank">简书</a></p><h3 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h3><p>语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- 列表内容</span><br><span class="line">+ 列表内容</span><br><span class="line">* 列表内容</span><br><span class="line"></span><br><span class="line">注意：- + * 跟内容之间都要有一个空格</span><br></pre></td></tr></table></figure></p><p>效果：</p><ul><li>列表1-</li><li>列表2-</li></ul><ul><li>列表1+</li><li>列表2+</li></ul><h3 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h3><p>语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">数字加点</span><br><span class="line"></span><br><span class="line">1.列表内容</span><br><span class="line">2.列表内容</span><br><span class="line">3.列表内容</span><br><span class="line"></span><br><span class="line">注意：序号跟内容之间要有空格</span><br></pre></td></tr></table></figure></p><p>例如：</p><ol><li>列表内容</li><li>列表内容</li><li>列表内容</li></ol><h3 id="列表嵌套"><a href="#列表嵌套" class="headerlink" title="列表嵌套"></a>列表嵌套</h3><p>语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上一级和下一级之间敲三个空格即可</span><br></pre></td></tr></table></figure></p><p>例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- 一级无序</span><br><span class="line">   - 二级无序</span><br><span class="line">   - 二级无序</span><br><span class="line">- 一级无序</span><br><span class="line">   1. 二级有序</span><br><span class="line">   2. 二级有序</span><br></pre></td></tr></table></figure></p><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g179t1gt8cj30eo04j3yd.jpg" alt></p><h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><p>语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">表头|表头|表头</span><br><span class="line">---|:--:|---:</span><br><span class="line">内容|内容|内容</span><br><span class="line">内容|内容|内容</span><br><span class="line"></span><br><span class="line">第二行分割表头和内容。</span><br><span class="line">- 有一个就行，为了对齐，多加了几个</span><br><span class="line">文字默认居左</span><br><span class="line">-两边加：表示文字居中</span><br><span class="line">-右边加：表示文字居右</span><br><span class="line">注：原生的语法两边都要用 | 包起来。此处省略</span><br></pre></td></tr></table></figure></p><p>在线生成HTML代码 <a href="http://www.tablesgenerator.com/" title="国外" target="_blank" rel="noopener">Tables Generator</a></p><p>例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">姓名|技能|排行</span><br><span class="line">--|:--:|--:</span><br><span class="line">刘备|哭|大哥</span><br><span class="line">关羽|打|二哥</span><br><span class="line">张飞|骂|三弟</span><br></pre></td></tr></table></figure></p><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g179vkbegbj30c804yjra.jpg" alt></p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>语法：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">单行代码:</span><br><span class="line">`int a;`</span><br></pre></td></tr></table></figure></p><p><code>int a;</code></p><p>代码块<br><img src="http://ww1.sinaimg.cn/large/0065saiygy1g17a0kot6hj30bq03cq2r.jpg" alt><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++)&#123;</span><br><span class="line">    a = a + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h3><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g17a3role7j30da08zjrt.jpg" alt></p><div id="flowchart-0" class="flow-chart"></div><div class="table-container"><table><thead><tr><th>6种类型</th><th>含义</th></tr></thead><tbody><tr><td>start</td><td>启动</td></tr><tr><td>end</td><td>结束</td></tr><tr><td>operation</td><td>程序</td></tr><tr><td>subroutine</td><td>子程序</td></tr><tr><td>condition</td><td>条件</td></tr><tr><td>inputoutput</td><td>输出</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>形参</th><th>实参</th><th>含义</th></tr></thead><tbody><tr><td>-&gt;</td><td>-&gt;</td><td>连接</td></tr><tr><td>condition</td><td>c1</td><td>条件</td></tr><tr><td>(布尔值,方向)</td><td>(yes,right)</td><td>如果满足向右连接，4种方向：right ，left，up ，down   默认为：down</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>形参</th><th>实参</th><th>含义</th></tr></thead><tbody><tr><td>tag</td><td>st</td><td>标签 (可以自定义)</td></tr><tr><td>=&gt;</td><td>=&gt;</td><td>赋值</td></tr><tr><td>type</td><td>start</td><td>类型 (6种类型)</td></tr><tr><td>content</td><td>开始</td><td>描述内容 (可以自定义)</td></tr><tr><td>:&gt;url</td><td><a href="http://www.baidu.com[blank" target="_blank" rel="noopener">http://www.baidu.com[blank</a>]</td><td>链接与跳转方式 兼容性很差</td></tr></tbody></table></div><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g17bm6ub5xj308b0600st.jpg" alt></p><div id="sequence-0"></div><div class="table-container"><table><thead><tr><th>符号</th><th>含义</th></tr></thead><tbody><tr><td>-</td><td>实线</td></tr><tr><td>&gt;</td><td>实心箭头</td></tr><tr><td>—</td><td>虚线</td></tr><tr><td>&gt;&gt;</td><td>空心箭头</td></tr></tbody></table></div><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g17a4umoxxj309602ugli.jpg" alt></p><div id="sequence-1"></div><h3 id="Latex数学公式"><a href="#Latex数学公式" class="headerlink" title="Latex数学公式"></a>Latex数学公式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">行内公式</span><br><span class="line">使用两个”$”符号引用公式</span><br><span class="line">$公式$</span><br><span class="line"></span><br><span class="line">行间公式</span><br><span class="line"></span><br><span class="line">使用两对“$$”符号引用公式：</span><br><span class="line">$$公式$$</span><br><span class="line"></span><br><span class="line">简单的规则：</span><br><span class="line">（1）空格：LaTeX中空格用来隔开单词(英语一类字母文字)，多个空格等效于一个空格；对中文没有作用。 </span><br><span class="line">（2）换行：用控制命令“\”,或“ \newline”. </span><br><span class="line">（3）分段：用控制命令“\par” 或空出一行。 </span><br><span class="line">（4）换页：用控制命令“\newpage”或“\clearpage” </span><br><span class="line">（5）特殊控制字符：#，$, %, &amp;, - ,&#123;, &#125;, ^, ~ </span><br><span class="line">\# \$ \% \&amp; \- \&#123; \&#125; \^&#123;&#125; \~&#123;&#125; $\backslash$表示“ \”.。</span><br></pre></td></tr></table></figure><p>常见数学符号</p><ol><li>指数和下标可以用^和_后加相应字符来实现。比如<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$a_&#123;1&#125;$ \qquad $x^&#123;2&#125;$ \qquad</span><br><span class="line">$e^&#123;-\alpha t&#125;$ \qquad</span><br><span class="line">$a^&#123;3&#125;_&#123;ij&#125;$\\</span><br><span class="line">$e^&#123;x^2&#125; \neq &#123;e^x&#125;^2$</span><br></pre></td></tr></table></figure></li></ol><script type="math/tex; mode=display">a_{1}</script><script type="math/tex; mode=display">x^{2}</script><script type="math/tex; mode=display">e^{-\alpha t}</script><script type="math/tex; mode=display">a^{3}_{ij}</script><ol><li>平方根<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$$\sqrt&#123;x&#125;$$</span><br><span class="line">$$\sqrt&#123; x^&#123;2&#125;+\sqrt&#123;y&#125; &#125;$$</span><br><span class="line">$$\sqrt[3]&#123;2&#125;$$</span><br><span class="line">$$\surd[x^2 + y^2]$$</span><br></pre></td></tr></table></figure></li></ol><script type="math/tex; mode=display">\sqrt{x}</script><script type="math/tex; mode=display">\sqrt{ x^{2}+\sqrt{y} }</script><script type="math/tex; mode=display">\sqrt[3]{2}</script><script type="math/tex; mode=display">\surd[x^2 + y^2]</script><ol><li>命令<code>\overline</code> 和<code>\underline</code> 在表达式的上、下方画出水平线。比如<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$$\overline&#123;m+n&#125;$$</span><br><span class="line">$$\underline&#123;m+n&#125;$$</span><br></pre></td></tr></table></figure></li></ol><script type="math/tex; mode=display">\overline{m+n}</script><script type="math/tex; mode=display">\underline{m+n}</script><ol><li>命令<code>\overbrace</code> 和<code>\underbrace</code> 在表达式的上、下方给出一水平的大括号。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$\underbrace&#123; a+b+\cdots+z &#125;_&#123;26&#125;$$</span><br></pre></td></tr></table></figure></li></ol><script type="math/tex; mode=display">\underbrace{ a+b+\cdots+z }_{26}</script><ol><li>其他<br>$\frac{1+x}{2^x}$</li></ol><script type="math/tex; mode=display">\sum_{i=0}N\int_{a}^{b}g(t,i)\text{d}t</script><script type="math/tex; mode=display">\begin{matrix}1&0&0\\0&1&0\\0&0&1\\\end{matrix}</script><script type="math/tex; mode=display">\begin{bmatrix}{a_{11}}&{a_{12}}&{\cdots}&{a_{1n}}\\{a_{21}}&{a_{22}}&{\cdots}&{a_{2n}}\\{\vdots}&{\vdots}&{\ddots}&{\vdots}\\{a_{m1}}&{a_{m2}}&{\cdots}&{a_{mn}}\\\end{bmatrix}</script><script type="math/tex; mode=display">\begin{cases}a_1x+b_1y+c_1z=d_1\\a_2x+b_2y+c_2z=d_2\\a_3x+b_3y+c_3z=d_3\\\end{cases}</script><script type="math/tex; mode=display">\begin{aligned}f_Y(y) & = f_X[h(y)]|h'(y)| \\[2ex]& = f_X[h(y)]h'(y) \\[2ex]& = \frac{1}{\theta}e^{-\frac{x}{\theta}}[\frac{dx}{dy}(-\frac{\theta}{ln(1-y)})] \\[2ex]& = \frac{1}{\theta}e^{-\frac{-\frac{\theta}{ln(1-y)}}{\theta}}\frac{\theta}{1-y} \\[2ex]& = \frac{1}{\theta}e^{ln(1-y)}\frac{\theta}{1-y} \\[2ex]& = \frac{1-y}{\theta}\frac{\theta}{1-y} \\[2ex]& = 1\end{aligned}</script><h3 id="锚点"><a href="#锚点" class="headerlink" title="锚点"></a>锚点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[锚点][锚点的定义]的 目标内容中不能有大写字母和空格，所以如果锚点目标的目标内容中有大写字母或空格，则需要在定义锚点中的目标内容时，</span><br><span class="line">把大写字母改成小写字母，</span><br><span class="line">把空格改成 -；</span><br><span class="line"></span><br><span class="line">[锚点][锚点的定义]的 目标内容 中不能含有以下字符：</span><br><span class="line">半角点(即英文中的句号).</span><br><span class="line"></span><br><span class="line">[回到顶部](##markdown_在hexo_中的渲染)</span><br><span class="line">[数学公式](#latex数学公式)</span><br></pre></td></tr></table></figure><p><a href="#markdown-在hexo-中的渲染">回到顶部</a><br><a href="#latex数学公式">数学公式</a></p><h3 id="diff语法"><a href="#diff语法" class="headerlink" title="diff语法"></a>diff语法</h3><p><img src="http://ww1.sinaimg.cn/large/0065saiygy1g17b18hbg1j303y0280si.jpg" alt><br><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="addition">+ new code</span></span><br><span class="line"><span class="deletion">- old code</span></span><br></pre></td></tr></table></figure></p><h3 id="自动邮箱连接"><a href="#自动邮箱连接" class="headerlink" title="自动邮箱连接"></a>自动邮箱连接</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;xxx@outlook.com&gt;</span><br></pre></td></tr></table></figure><p><a href="&#x6d;&#x61;&#105;&#108;&#116;&#111;&#58;&#120;&#120;&#x78;&#64;&#x6f;&#x75;&#116;&#x6c;&#111;&#x6f;&#x6b;&#46;&#x63;&#111;&#109;">&#120;&#120;&#x78;&#64;&#x6f;&#x75;&#116;&#x6c;&#111;&#x6f;&#x6b;&#46;&#x63;&#111;&#109;</a></p><h3 id="脚注-2"><a href="#脚注-2" class="headerlink" title="脚注  2"></a>脚注  <sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup></h3><p>代码：<br><img src="http://ww1.sinaimg.cn/large/0065saiygy1g17u5ykm67j30jf06cweo.jpg" alt><br>效果：<br><img src="http://ww1.sinaimg.cn/large/0065saiygy1g17u8tcrv0j30e40bc0su.jpg" alt></p><p><script src="https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js"></script><textarea id="flowchart-0-code" style="display: none">st=>start: Start|past:>http://www.google.com[blank]e=>end: End:>http://www.google.comop1=>operation: My Operation|pastop2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalidcond=>condition: Yesor No?|approved:>http://www.google.comc2=>condition: Good idea|rejectedio=>inputoutput: catch something...|requestst->op1(right)->condcond(yes, right)->c2cond(no)->sub1(left)->op1c2(yes)->io->ec2(no)->op2->e</textarea><textarea id="flowchart-0-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script><script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.27/webfontloader.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/snap.svg/0.4.1/snap.svg-min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.8.3/underscore-min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/js-sequence-diagrams/1.0.6/sequence-diagram-min.js"></script><textarea id="sequence-0-code" style="display: none">    起床->吃饭: 稀饭油条    吃饭->上班: 不要迟到了    上班->午餐: 吃撑了    上班->下班:    Note right of 下班: 下班了    下班->回家:    Note right of 回家: 到家了    回家-->>起床:    Note left of 起床: 新的一天</textarea><textarea id="sequence-0-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("sequence-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-0-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-0", options);</script><textarea id="sequence-1-code" style="display: none">Alice->Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob-->Alice: I am good thanks!</textarea><textarea id="sequence-1-options" style="display: none">{"theme":"simple","scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("sequence-1-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("sequence-1-options").value));  var diagram = Diagram.parse(code);  diagram.drawSVG("sequence-1", options);</script></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;整理的markdown语法，以及在hexo中的特殊渲染&lt;br&gt;
    
    </summary>
    
      <category term="markdown" scheme="https://www.smartzhi.site/categories/markdown/"/>
    
    
      <category term="markdown" scheme="https://www.smartzhi.site/tags/markdown/"/>
    
      <category term="hexo" scheme="https://www.smartzhi.site/tags/hexo/"/>
    
  </entry>
  
</feed>
